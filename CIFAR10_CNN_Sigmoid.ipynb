{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10_CNN_Sigmoid.ipynb","provenance":[{"file_id":"1sz3a1vF_7vcUEte6iG0o__MyVlMKCp7S","timestamp":1615876133525}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HxJYuaOPAmPv"},"source":["import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELP-dOZIWd1s"},"source":["# Tensorflow Colab file with some modifications"]},{"cell_type":"code","metadata":{"id":"6du4sCo7Wifx"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cat1JGJYWnY8"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-C9Zc2lWqx6"},"source":["intBits = 1\n","precision = 17\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='sigmoid', input_shape=(32, 32, 3)))\n","#model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c3'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='sigmoid'))\n","#model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        name='d1'))\n","#model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","model.add(layers.Dense(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJfLC9yrWu3Z"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs=10, \n","                    validation_data=(test_images[:5000], test_labels[:5000]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw5LYwqdbR54"},"source":["import qkeras\n","from qkeras import *\n","from qkeras.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86ytLEYSbZ0e"},"source":["model_save_quantized_weights(model)\n","print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x89GmBLzbfXZ"},"source":["model.evaluate(test_images[5000:], test_labels[5000:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgG4J9qoeE3n"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","for precision in [8, 12, 16, 24, 32]:\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(Activation('sigmoid'))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images, train_labels, epochs= 5 + int(3*((precision/4) - 2)), \n","                    validation_data=(test_images[:5000], test_labels[:5000]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images[5000:], test_labels[5000:])\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktoOQ5tjRWaY"},"source":["# Now with CORDIC"]},{"cell_type":"code","metadata":{"id":"hyPGZBBJRV1N"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  yarr = np.zeros(shape = (len(arr), len(arr[0])))\n","\n","  global  lookup_arctanh\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","    \n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","    \n","  ex = xarr + yarr\n","\n","  #temp = quantized_bits(precision, 1, alpha=1)(1 + ex)\n","  return quantized_bits(precision,1, alpha=1)(ex/(1+ex))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rO1XQMfSDRl"},"source":["qmodel = 0 \n","\n","def epicGeneratePredictions(indices, precision, iterations):\n","  global qmodel\n","  x_test = test_images\n","  from keras import backend as K\n","  get_sixth_layer_output = K.function([qmodel.layers[0].input],\n","                                    [qmodel.layers[6].output])\n"," \n","  layer6_output = get_sixth_layer_output([x_test[indices[0]: indices[1]]])[0]\n","\n","  #layer6_output = layer6_output[indices[0]: indices[1]]\n","\n","  layer7_output = modifiedCordicTanh(layer6_output, precision, iterations)\n","\n","  input_shape = qmodel.layers[8].get_input_shape_at(0)\n","  layer_input = Input(shape=(64))\n","  x = layer_input\n","  x = qmodel.layers[8](x)\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(layer7_output))\n","\n","  #predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  a = predictions\n","  return (a == a.max(axis=1)[:,None]).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9euEXUKqSJjc"},"source":["def modelMaker(precision, intBits, epoch=10):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(Activation('sigmoid'))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images, train_labels, epochs= epoch, \n","                    validation_data=(test_images[:5000], test_labels[:5000]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOapip90SxvV"},"source":["## 8bit"]},{"cell_type":"code","metadata":{"id":"U-1NAzvWSPYG"},"source":["qmodel = modelMaker(9, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 9, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNSpargZbkIQ"},"source":["## 12 bit"]},{"cell_type":"code","metadata":{"id":"2wVx7wlDbjpe"},"source":["qmodel = modelMaker(13, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 13, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYtM_IGQSOgG"},"source":["## 16 bit"]},{"cell_type":"code","metadata":{"id":"FXjr28BvSQbW"},"source":["qmodel = modelMaker(17, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 17, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKiKLCfJTBxu"},"source":["## 24 bit"]},{"cell_type":"code","metadata":{"id":"SHIu_ZHyTDuR"},"source":["\n","for epochs in range(8, 15):\n","  qmodel = modelMaker(33, 1, epochs)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([5000, 10000], 33, iter))\n","    print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[5000:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(5000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uB-oGaiVAzc"},"source":["## 32 Bit"]},{"cell_type":"code","metadata":{"id":"KK2bYJ25VCgC"},"source":["qmodel = modelMaker(33, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 33, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]}]}