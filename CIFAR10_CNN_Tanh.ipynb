{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10_CNN_Tanh.ipynb","provenance":[{"file_id":"1sz3a1vF_7vcUEte6iG0o__MyVlMKCp7S","timestamp":1615879820365}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HxJYuaOPAmPv"},"source":["import multiprocessing \n","\n","import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELP-dOZIWd1s"},"source":["# Tensorflow Colab file with some modifications"]},{"cell_type":"code","metadata":{"id":"6du4sCo7Wifx"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cat1JGJYWnY8"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])\n","#train_labels, test_labels = get_one_hot(train_labels, 10), get_one_hot(test_labels, 10)#"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaEdKdcGojSp"},"source":["test_labels[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-C9Zc2lWqx6"},"source":["intBits = 1\n","precision = 17\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='tanh', input_shape=(32, 32, 3)))\n","#model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='tanh'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='tanh'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c3'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='tanh'))\n","#model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        name='d1'))\n","#model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","model.add(layers.Dense(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJfLC9yrWu3Z"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs=10, \n","                    validation_data=(test_images[:5000], test_labels[:5000]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw5LYwqdbR54"},"source":["import qkeras\n","from qkeras import *\n","from qkeras.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86ytLEYSbZ0e"},"source":["model_save_quantized_weights(model)\n","print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x89GmBLzbfXZ"},"source":["model.evaluate(test_images[5000:], test_labels[5000:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgG4J9qoeE3n"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","for precision in [8, 12, 16, 24, 32]:\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images, train_labels, epochs= 5 + int(3*((precision/4) - 2)), \n","                    validation_data=(test_images[:5000], test_labels[:5000]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images[5000:], test_labels[5000:])\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcTO8AQxCVTT"},"source":["mymodels[3].layers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-fHgcFxBkWS"},"source":["# Now we go for CORDIC "]},{"cell_type":"code","metadata":{"id":"gJX3XfYHBn_q"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  yarr = np.zeros(shape = (len(arr), len(arr[0])))\n","\n","  global  lookup_arctanh\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","    \n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","  return quantized_bits(precision, 1, alpha=1)(yarr / xarr)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5gu-8WLCAXv"},"source":["qmodel = 0 \n","\n","def epicGeneratePredictions(indices, precision, iterations):\n","  global qmodel\n","  x_test = test_images\n","  from keras import backend as K\n","  get_sixth_layer_output = K.function([qmodel.layers[0].input],\n","                                    [qmodel.layers[6].output])\n"," \n","  layer6_output = get_sixth_layer_output([x_test])[0]\n","\n","  layer6_output = layer6_output[indices[0]: indices[1]]\n","\n","  layer7_output = modifiedCordicTanh(layer6_output, precision, iterations)\n","\n","  input_shape = qmodel.layers[8].get_input_shape_at(0)\n","  layer_input = Input(shape=(64))\n","  x = layer_input\n","  x = qmodel.layers[8](x)\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(layer7_output))\n","\n","  #predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  a = predictions\n","  return (a == a.max(axis=1)[:,None]).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxSrKO_fFqgc"},"source":["def modelMaker(precision, intBits):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images, train_labels, epochs= 5 + int(3*(precision/4 - 2)), \n","                    validation_data=(test_images[:5000], test_labels[:5000]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u65j3YvrEtc5"},"source":["## 8bit"]},{"cell_type":"code","metadata":{"id":"P0LgZum2EqgJ"},"source":["qmodel = modelMaker(9, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 9, iter))\n","  print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wE6v7ipJNtZk"},"source":["accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQrjTsGWtXZJ"},"source":["## 12 bit"]},{"cell_type":"code","metadata":{"id":"t9-bsjjctZy0"},"source":["qmodel = modelMaker(13, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 13, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wLpXMAkuQ1D"},"source":["## 16 bit"]},{"cell_type":"code","metadata":{"id":"T9YhLNeHuNWB"},"source":["qmodel = modelMaker(17, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 17, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ur-znNAwvA8h"},"source":["## 24 bit"]},{"cell_type":"code","metadata":{"id":"PXKt6zNgvDMz"},"source":["qmodel = modelMaker(25, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 25, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSedv_upzjW6"},"source":["## 32 Bit"]},{"cell_type":"code","metadata":{"id":"t1QD3sZ0zh1L"},"source":["qmodel = modelMaker(33, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([5000, 10000], 33, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[5000:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(5000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]}]}