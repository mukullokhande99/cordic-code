{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10_TanhPWLAF.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ieqrwF1uUTQ3"},"source":["Here's what I will do. \n","\n","For precision n\n","\n","I'll map an inp to activation layer lying in the range x , x + 2^(-n) to a value y. Both x and y will have precision 2^(-n). How to decide upon 'y' ? \n","\n","We will choose that y which is closest  to avg of AF(x) and AF(x + 2^-n)"]},{"cell_type":"code","metadata":{"id":"X9llfRIqVVvh"},"source":["import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import keras\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAqg84oSVY8q"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MsEz3KzpV8tj"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcHPyPVLXNFI"},"source":["# Tanh Intervals and Mappings"]},{"cell_type":"markdown","metadata":{"id":"GfKyAmSHXRtG"},"source":["## 8 bit"]},{"cell_type":"code","metadata":{"id":"Ek5WqZsbXA6P"},"source":["def swish(x):\n","  global intervals, coeffArray\n","  #return x\n","  #coeff = np.array(coeffArray)\n","  #conditionArray = sum([tf.multiply(tf.cast(tf.math.logical_and(tf.math.less(x, 0.03125*(n+1)), tf.math.greater_equal(x, 0.03125*n)), tf.float32), coeff[n][0]*x + coeff[n][1]*K.ones_like(x)) for n in range(256)])\n","\n","  #return quantized_bits(17, 1)(conditionArray)\n","  #return conditionArray\n","\n","  quantizer = quantized_bits(precision+1, 1)\n","  xprime = quantizer(x)\n","  argument = 0.5*(tf.math.tanh(xprime) + tf.math.tanh(xprime + 2**(-precision)))\n","  return quantizer(argument)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tvt3G3vUWK3a"},"source":["intBits = 1\n","precision = 32\n","\n","model = models.Sequential()\n","#model.add(layers.Conv2D(32, (3, 3), activation='sigmoid', input_shape=(32, 32, 3)))\n","model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation=\"quantized_tanh({}, {})\".format(precision, intBits),\n","        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","#model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation=\"quantized_tanh({}, {})\".format(precision, intBits),\n","        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","#model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation=\"quantized_tanh({}, {})\".format(precision, intBits),\n","        name='c3'))\n","\n","model.add(layers.Flatten())\n","#model.add(layers.Dense(64, activation='sigmoid'))\n","model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","model.add(Activation(swish))\n","model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name = 'd2'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGCqbjRy76rQ"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs= 13, batch_size=256,\n","              validation_data=(test_images[:5000], test_labels[:5000]), verbose=True)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM0niIkTF7ug"},"source":["model.evaluate(test_images[5000:], test_labels[5000:])"],"execution_count":null,"outputs":[]}]}