{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF059Fd8p5WS",
        "outputId": "46d28b4e-66b3-419a-fce2-fb287aa34d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/google/qkeras\n",
            "  Cloning https://github.com/google/qkeras to /tmp/pip-req-build-6hv98wef\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/qkeras /tmp/pip-req-build-6hv98wef\n",
            "  Resolved https://github.com/google/qkeras to commit 6be2f5ca75d9a42209b17f42e86087f6d60cf961\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (1.11.4)\n",
            "Collecting pyparser (from QKeras==0.9.0)\n",
            "  Downloading pyparser-1.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (67.7.2)\n",
            "Collecting tensorflow-model-optimization>=0.2.1 (from QKeras==0.9.0)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (3.3)\n",
            "Collecting keras-tuner>=1.0.1 (from QKeras==0.9.0)\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from QKeras==0.9.0) (4.66.4)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner>=1.0.1->QKeras==0.9.0) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner>=1.0.1->QKeras==0.9.0) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner>=1.0.1->QKeras==0.9.0) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner>=1.0.1->QKeras==0.9.0)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.1->QKeras==0.9.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.1->QKeras==0.9.0) (3.5.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.9.0) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.9.0) (0.1.8)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.9.0) (1.16.0)\n",
            "Collecting parse==1.6.5 (from pyparser->QKeras==0.9.0)\n",
            "  Downloading parse-1.6.5.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.9.0) (2024.2.2)\n",
            "Building wheels for collected packages: QKeras, pyparser, parse\n",
            "  Building wheel for QKeras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for QKeras: filename=QKeras-0.9.0-py3-none-any.whl size=175446 sha256=eec77953fe90fb2768acef0bdc7671450ea1279d18b75ec7c6a9dcb056401eee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmqn2267/wheels/9e/fd/c9/9e660ece157054c7d9225b4ce0e010e5bfa4812a349c96d247\n",
            "  Building wheel for pyparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyparser: filename=pyparser-1.0-py3-none-any.whl size=4915 sha256=c1c65a6aa7e9c36fbe0627d55f4a16763269202aaf9411998c6a578ec9c3978a\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/87/78/ff95e8a747dc534fbd199fb3ea06d80935bc87e44567bbdb31\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.6.5-py3-none-any.whl size=18155 sha256=ffa05df22947f884f32245c5669bfdf67734a6c2230f85e494e733872f90c00e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/10/c8/5023ea72290855ee33e7bc403e1048ae238b9c2fdb549a9f51\n",
            "Successfully built QKeras pyparser parse\n",
            "Installing collected packages: parse, kt-legacy, tensorflow-model-optimization, pyparser, keras-tuner, QKeras\n",
            "Successfully installed QKeras-0.9.0 keras-tuner-1.4.7 kt-legacy-1.0.5 parse-1.6.5 pyparser-1.0 tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ],
      "source": [
        "import six\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "!pip install git+https://github.com/google/qkeras\n",
        "from qkeras.quantizers import quantized_bits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
        "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
        "\n",
        "    x_train /= 256.0\n",
        "    x_test /= 256.0\n",
        "\n",
        "    x_mean = np.mean(x_train, axis=0)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_test -= x_mean\n",
        "\n",
        "    nb_classes = np.max(y_train)+1\n",
        "    y_train = to_categorical(y_train, nb_classes)\n",
        "    y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "    # # for int 4\n",
        "    # quantizer = quantized_bits(4, 1)\n",
        "    # # for int 8\n",
        "    # quantizer = quantized_bits(8, 1)\n",
        "    # # for int 12\n",
        "    # quantizer = quantized_bits(12, 1)\n",
        "    # # for int 16\n",
        "    # quantizer = quantized_bits(16, 1)\n",
        "    # For int 32\n",
        "    quantizer = quantized_bits(32, 1)\n",
        "\n",
        "\n",
        "    x_train = quantizer(x_train).numpy()\n",
        "    x_test = quantizer(x_test).numpy()\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao_YNYgIqKfE",
        "outputId": "a2b52e9d-6c87-4ee5-d3b7-040b433ccfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras import backend as K\n",
        "# class MyActivation(keras.layers.Layer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(MyActivation, self).__init__(**kwargs)\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         # Create a trainable weight variable for this layer.\n",
        "#         self.a = self.add_weight(name='a',\n",
        "#                                       shape=(1),\n",
        "#                                       initializer ='ones',  # TODO: Choose your initializer\n",
        "#                                       trainable=True)\n",
        "#         self.c = self.add_weight(name='c',\n",
        "#                                       shape=(1),\n",
        "#                                       initializer='ones',  # TODO: Choose your initializer\n",
        "#                                       trainable=True)\n",
        "#         super(MyActivation, self).build(input_shape)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         #return tf.matmul(inputs, self.w) + self.b\n",
        "#         #quantizer = quantized_bits(9, 1)\n",
        "#         return K.maximum(self.a*(x + self.c), K.minimum(self.a*(x - self.c) + self.c, x))\n",
        "#         #return K.maximum(K.zeros_like(x), x)\n",
        "\n",
        "#     def compute_output_shape(self, input_shape):\n",
        "#         return input_shape"
      ],
      "metadata": {
        "id": "v6bdk_IiqMDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras import backend as K\n",
        "# def swish(x, beta=10.0):\n",
        "#     #Here return the c1*x + c2\n",
        "#     global intervals, coeffArray\n",
        "#     #idx = np.array(intervals).searchsorted(x.data())\n",
        "#     idx = quantized_bits(9,1)(x)/0.03125\n",
        "#     coeff = np.array(coeffArray)\n",
        "#     return coeff[idx][0]*x + coeff[idx][1]*K.ones_like(x)\n",
        "#     a = tf.gather(coeff, int(idx))\n",
        "#     return x\n",
        "\n",
        "#     #K.stop_gradient()"
      ],
      "metadata": {
        "id": "6KBNyVVJqMli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Here I'll try to use the method outlined in this link\n",
        "# # https://stackoverflow.com/questions/54204393/piecewise-activation-function-in-tensorflow-and-broadcasting-math-operation\n",
        "\n",
        "# def swish(x):\n",
        "#   global intervals, coeffArray\n",
        "#   coeff = np.array(coeffArray)\n",
        "#   conditionArray = sum([tf.multiply(tf.cast(tf.math.logical_and(tf.math.less(x, 0.03125*(n+1)), tf.math.greater_equal(x, 0.03125*n)), tf.float32), coeff[n][0]*x + coeff[n][1]*K.ones_like(x)) for n in range(256)])\n",
        "\n",
        "#   return conditionArray"
      ],
      "metadata": {
        "id": "FLPD3V1mqPAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below, uncomment the block according to whether int4, int8, int12 or int16 or fp32"
      ],
      "metadata": {
        "id": "vDPOsX8kqW2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "\n",
        "\n",
        "#for int4\n",
        "# def CreateModel(shape, nb_classes, intBits):\n",
        "#     x = x_in = Input(shape)\n",
        "#     x = Flatten(name=\"flatten\")(x)\n",
        "\n",
        "#     x = QDense(256,\n",
        "#         kernel_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense2\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense3\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(nb_classes,\n",
        "#         kernel_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(4, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense4\")(x)\n",
        "\n",
        "#     # Output layer with softmax activation\n",
        "#     x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "#     model = Model(inputs=x_in, outputs=x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "#For 8 bits\n",
        "# def CreateModel(shape, nb_classes, intBits):\n",
        "#     x = x_in = Input(shape)\n",
        "#     x = Flatten(name=\"flatten\")(x)\n",
        "\n",
        "#     x = QDense(256,\n",
        "#         kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(9, {} , alpha = 1)\".format(intBits),\n",
        "#         name=\"dense\")(x)\n",
        "\n",
        "#     #x = MyActivation()(x)\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense2\")(x)\n",
        "\n",
        "#     #x = MyActivation()(x)\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense3\")(x)\n",
        "\n",
        "#     #x = MyActivation()(x)\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(nb_classes,\n",
        "#         kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense4\")(x)\n",
        "\n",
        "#     x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "#     model = Model(inputs=x_in, outputs=x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# for int 12\n",
        "# def CreateModel(shape, nb_classes, intBits):\n",
        "#     x = x_in = Input(shape)\n",
        "#     x = Flatten(name=\"flatten\")(x)\n",
        "\n",
        "#     x = QDense(256,\n",
        "#         kernel_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense2\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense3\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(nb_classes,\n",
        "#         kernel_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(12, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense4\")(x)\n",
        "\n",
        "#     # Output layer with softmax activation\n",
        "#     x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "#     model = Model(inputs=x_in, outputs=x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "#For 16 bits\n",
        "# def CreateModel(shape, nb_classes, intBits):\n",
        "#     x = x_in = Input(shape)\n",
        "#     x = Flatten(name=\"flatten\")(x)\n",
        "\n",
        "#     x = QDense(256,\n",
        "#         kernel_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(16, {} , alpha = 1)\".format(intBits),\n",
        "#         name=\"dense\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense2\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(128,\n",
        "#         kernel_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense3\")(x)\n",
        "\n",
        "#     # Activation layer\n",
        "#     #x = Activation(swish)(x)\n",
        "#     x = Activation('relu')(x)\n",
        "\n",
        "#     x = QDense(nb_classes,\n",
        "#         kernel_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         bias_quantizer=\"quantized_bits(16, {} , alpha=1)\".format(intBits),\n",
        "#         name=\"dense4\")(x)\n",
        "\n",
        "#     # Output layer with softmax activation\n",
        "#     x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "#     model = Model(inputs=x_in, outputs=x)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# For 32 bits\n",
        "def CreateModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(32, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "\n",
        "    # Activation layer\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "\n",
        "    # Activation layer\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "\n",
        "    # Activation layer\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(32, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "rIYiOVNXqTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "\n",
        "model = CreateModel(x_train.shape[1:], y_train.shape[-1], 1)\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=Adam(0.0005),\n",
        "    #optimizer='sgd',\n",
        "    metrics=[\"accuracy\"],)\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False, callbacks=[callback])"
      ],
      "metadata": {
        "id": "9z_fpkHPqfUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test[5000:], y_test[5000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrTPkxqBqk_G",
        "outputId": "d035d730-4441-4284-c9c9-e15ae0212793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 1s 5ms/step - loss: 0.0457 - accuracy: 0.9850\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04570109024643898, 0.9850000143051147]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we get the test accuracy here !"
      ],
      "metadata": {
        "id": "5CZ7X-XsqpVq"
      }
    }
  ]
}