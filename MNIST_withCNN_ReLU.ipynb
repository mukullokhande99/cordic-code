{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_withCNN_ReLU.ipynb","provenance":[{"file_id":"1sz3a1vF_7vcUEte6iG0o__MyVlMKCp7S","timestamp":1616692915476}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HxJYuaOPAmPv"},"source":["import multiprocessing \n","\n","import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClgDKJvSacGi"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOmxo96CBcCy"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])\n","#train_labels, test_labels = get_one_hot(train_labels, 10), get_one_hot(test_labels, 10)#\n","\n","train_images = train_images.reshape(train_images.shape + (1,)).astype(\"float32\")\n","test_images = test_images.reshape(test_images.shape + (1,)).astype(\"float32\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEEV7FxHBc4Z"},"source":["from qkeras import *\n","import qkeras\n","from qkeras.utils import *\n","\n","def CreateQModel(shape, nb_classes, intBits):\n","    x = x_in = Input(shape)\n","\n","    x = QConv2D(32, (3, 3),\n","                kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","                bias_quantizer=\"quantized_bits(17, {} , alpha = 1)\".format(intBits),\n","                name='c1')(x)\n","    x = QActivation('quantized_relu(17, {})'.format(intBits))(x)\n","    x = MaxPooling2D((2,2))(x)\n","\n","    x = QConv2D(64, (3, 3),\n","                kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","                bias_quantizer=\"quantized_bits(17, {} , alpha = 1)\".format(intBits),\n","                name=\"c2\")(x)\n","    x = QActivation('quantized_relu(17, {})'.format(intBits))(x)\n","    x = MaxPooling2D((2,2))(x)\n","\n","    x = QConv2D(64, (3, 3),\n","                kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","                bias_quantizer=\"quantized_bits(17, {} , alpha = 1)\".format(intBits),\n","                name='c3')(x)\n","    x = QActivation('quantized_relu(17, {})'.format(intBits))(x)\n","\n","    x = Flatten(name=\"flatten\")(x)\n","\n","    x = QDense(64,\n","        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","        bias_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","        name=\"dense3\")(x)\n","    x = QActivation('quantized_relu(17, {})'.format(intBits))(x)\n","\n","    x = QDense(nb_classes,\n","        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","        bias_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n","        name=\"dense4\")(x)\n","    x = Activation(\"softmax\", name=\"softmax\")(x)\n","\n","\n","    model = Model(inputs=x_in, outputs=x)\n","    \n","    return model\n","\n","from tensorflow.keras.optimizers import Adam\n","model = CreateQModel(x_train.shape[1:], y_train.shape[-1], 1)\n","model.compile(\n","    loss=\"categorical_crossentropy\",\n","    #loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=Adam(0.1),\n","    #optimizer='sgd',\n","    metrics=[\"accuracy\"])\n","history = model.fit(x_train, y_train, epochs=10, batch_size=256, validation_data=(x_test[:5000], y_test[:5000]), verbose=True) # callbacks=[callback])\n","model_save_quantized_weights(model)\n","print (\"Done\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DdF6i-ZOPn4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELP-dOZIWd1s"},"source":["# Tensorflow Colab file with some modifications"]},{"cell_type":"code","metadata":{"id":"6du4sCo7Wifx"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cat1JGJYWnY8"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-C9Zc2lWqx6"},"source":["intBits = 1\n","precision = 32\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n","#model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c3'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","#model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        name='d1'))\n","#model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","model.add(layers.Activation('relu'))\n","model.add(layers.Dense(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJfLC9yrWu3Z"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images[:48000], train_labels[:48000], epochs=15, \n","                    validation_data=(train_images[48000:], train_labels[48000:]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw5LYwqdbR54"},"source":["import qkeras\n","from qkeras import *\n","from qkeras.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86ytLEYSbZ0e"},"source":["model_save_quantized_weights(model)\n","print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x89GmBLzbfXZ"},"source":["model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgG4J9qoeE3n"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","for precision in [25]:\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","  model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d2'))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= 3 + int((precision/4) - 2), \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images, test_labels)\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0z8ZcgcdTNJ"},"source":["histories[4].history['val_accuracy']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gc9QBarnvRz_"},"source":["# Only for 24 bit"]},{"cell_type":"code","metadata":{"id":"yLfaEPYMvTgY"},"source":["### Evaluation in a loop\n","precision = 25\n","intBits = 1\n","histories = []\n","mymodels = []\n","for epoch in range(3, 15):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_relu({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","  model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d2'))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= epoch, \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images, test_labels)\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]}]}