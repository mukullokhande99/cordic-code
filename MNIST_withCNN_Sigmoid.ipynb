{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_withCNN_Sigmoid.ipynb","provenance":[{"file_id":"1UqCmS-KK6x12inCzAcGVLmpMEhSG-tIb","timestamp":1616726465891},{"file_id":"1sz3a1vF_7vcUEte6iG0o__MyVlMKCp7S","timestamp":1615876133525}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HxJYuaOPAmPv"},"source":["import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELP-dOZIWd1s"},"source":["# Tensorflow Colab file with some modifications"]},{"cell_type":"code","metadata":{"id":"dPUWGF8_Zib_"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *\n","\n","(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])\n","#train_labels, test_labels = get_one_hot(train_labels, 10), get_one_hot(test_labels, 10)#\n","\n","train_images = train_images.reshape(train_images.shape + (1,)).astype(\"float32\")\n","test_images = test_images.reshape(test_images.shape + (1,)).astype(\"float32\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6du4sCo7Wifx"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cat1JGJYWnY8"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-C9Zc2lWqx6"},"source":["intBits = 1\n","precision = 17\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='sigmoid'))\n","#model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='sigmoid'))\n","#model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        activation='quantized_relu({}, {})'.format(precision, intBits),\n","#        name='c3'))\n","\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='sigmoid'))\n","#model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","#        name='d1'))\n","#model.add(QActivation('quantized_relu({}, {})'.format(precision, intBits)))\n","model.add(layers.Dense(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJfLC9yrWu3Z"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs=10, \n","                    validation_data=(test_images[:5000], test_labels[:5000]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw5LYwqdbR54"},"source":["import qkeras\n","from qkeras import *\n","from qkeras.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86ytLEYSbZ0e"},"source":["model_save_quantized_weights(model)\n","print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x89GmBLzbfXZ"},"source":["model.evaluate(test_images[5000:], test_labels[5000:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgG4J9qoeE3n"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","for precision in [32]:\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c1'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c2'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c3'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(Activation('sigmoid'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= 20, \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images, test_labels)\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QJZkn3kdMbH"},"source":["histories[4].history['val_accuracy']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktoOQ5tjRWaY"},"source":["# Now with CORDIC"]},{"cell_type":"code","metadata":{"id":"hyPGZBBJRV1N"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, intBits, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  yarr = np.zeros(shape = (len(arr), len(arr[0])))\n","\n","  global  lookup_arctanh\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","    \n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","    \n","  ex = xarr + yarr\n","\n","  #temp = quantized_bits(precision, 1, alpha=1)(1 + ex)\n","  return quantized_bits(precision,intBits, alpha=1)(ex/(1+ex))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fEnNQmbK2LS"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, intBits, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  yarr = np.zeros(shape = (len(arr), len(arr[0])))\n","  arr = -1*arr\n","  global  lookup_arctanh\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","    \n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","    \n","  ex = xarr + yarr\n","\n","  #temp = quantized_bits(precision, 1, alpha=1)(1 + ex)\n","  return quantized_bits(precision,intBits, alpha=1)(1/(1+ex))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryEjqiQtOsoZ"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(x, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  \n","\n","  global  lookup_arctanh\n","  current_vector = np.array([1.2075, 0])\n","  z = x\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(z)\n","    x = current_vector[0]\n","    y = current_vector[1]\n","    xnew = x\n","    ynew = y\n","    xnew = xnew - m*sigma*2**(-i)*y\n","    ynew = ynew + sigma*2**(-i)*x\n","    z = z - sigma*lookup_arctanh[i-1]\n","    current_vector = [xnew, ynew]\n","\n","  \n","  ex = current_vector[0] + current_vector[1]\n","  eminusx = current_vector[0] - current_vector[1]\n","\n","  temp1 = quantized_bits(9,1,alpha=1)(ex - eminusx)\n","  temp2 = quantized_bits(9, 1, alpha=1)(ex + eminusx)\n","\n","  return quantized_bits(9,1, alpha=1)(temp1/temp2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rO1XQMfSDRl"},"source":["qmodel = 0 \n","\n","def epicGeneratePredictions(indices, precision, intBits, iterations):\n","  global qmodel\n","  x_test = test_images\n","  from keras import backend as K\n","  get_sixth_layer_output = K.function([qmodel.layers[0].input],\n","                                    [qmodel.layers[6].output])\n"," \n","  layer6_output = get_sixth_layer_output([x_test[indices[0]: indices[1]]])[0]\n","\n","  #layer6_output = layer6_output[indices[0]: indices[1]]\n","\n","  layer7_output = modifiedCordicTanh(layer6_output, precision, intBits, iterations)\n","\n","  input_shape = qmodel.layers[8].get_input_shape_at(0)\n","  layer_input = Input(shape=(64))\n","  x = layer_input\n","  x = qmodel.layers[8](x)\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(layer7_output))\n","\n","  #predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  a = predictions\n","  return (a == a.max(axis=1)[:,None]).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9euEXUKqSJjc"},"source":["def modelMaker(precision, intBits, epoch=10):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(Activation('sigmoid'))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epoch, \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False, callbacks=[callback])\n","  \n","  model_save_quantized_weights(model)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji1_U_1Calcx"},"source":["Epochs:\n","\n","for 8bit: 5 + int(precision/4)\n","\n","for 12 bit: 10 + \"\n","\n","for 16 bit: "]},{"cell_type":"markdown","metadata":{"id":"rOapip90SxvV"},"source":["## 8bit"]},{"cell_type":"code","metadata":{"id":"U-1NAzvWSPYG"},"source":["# # 1/ 1 + e-x\n","\n","for epoch in range(5, 15):\n","  qmodel = modelMaker(9, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 9,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ik6PaR-M7G3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNSpargZbkIQ"},"source":["## 12 bit"]},{"cell_type":"code","metadata":{"id":"JW-oOElNCxVl"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(13, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 13,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wVx7wlDbjpe"},"source":["qmodel = modelMaker(13, 1, 11)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 13, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYtM_IGQSOgG"},"source":["## 16 bit"]},{"cell_type":"code","metadata":{"id":"WWQmrzIhEvmW"},"source":["for epoch in range(3, 20):\n","  qmodel = modelMaker(17, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 17,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXjr28BvSQbW"},"source":["qmodel = modelMaker(17, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 17, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKiKLCfJTBxu"},"source":["## 24 bit"]},{"cell_type":"code","metadata":{"id":"SHIu_ZHyTDuR"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(24, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 24,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFxRwU6ThRJj"},"source":["for epoch in range(15, 20):\n","  qmodel = modelMaker(25, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 25,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uB-oGaiVAzc"},"source":["## 32 Bit"]},{"cell_type":"code","metadata":{"id":"7VeKONI-p0H0"},"source":["for epoch in range(3, 20):\n","  qmodel = modelMaker(33, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 33,1, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KK2bYJ25VCgC"},"source":["qmodel = modelMaker(33, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 33, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkkH9dyEfPPh"},"source":["for epoch in range"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMgUp6A8c-bT","executionInfo":{"status":"ok","timestamp":1616760807130,"user_tz":-330,"elapsed":1430,"user":{"displayName":"Ribhu Das Purkayastha","photoUrl":"","userId":"17175187423228014746"}},"outputId":"ecfd5cc6-5925-4aec-d904-d781ffeab82c"},"source":["qmodel.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.1153 - accuracy: 0.9680\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.11529628932476044, 0.9679999947547913]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"5sds7227fo95"},"source":["accuracies = []\n","for epoch in range(5, 20):\n","\n","  qmodel = modelMaker(33, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 33, iter))\n","    print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  accuracies.append(accuracy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH5MsBD3hcfm"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVEXqXDmxyZJ"},"source":["# Varying Binary Point"]},{"cell_type":"markdown","metadata":{"id":"oEyptPigqmTI"},"source":["## 8 bit"]},{"cell_type":"code","metadata":{"id":"zi5rQobbx2mb"},"source":["# Here we take 8bit sigmoid...and vary the binary point and determine the accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgqfNfooyWIi"},"source":["accuracies = []\n","for intBits in range(9):\n","  epochAccuracy = []\n","  for epoch in range(5, 20):\n","    qmodel = modelMaker(9, intBits, epoch)\n","    answerVectors = []\n","    for iter in [2,3,4, 5, 6, 7, 8]:\n","      answerVectors.append(epicGeneratePredictions([0, 10000], 9, intBits, iter))\n","      #print (\"Done\")\n","\n","    accuracy = []\n","    y_test = get_one_hot(test_labels[0:10000], 10)\n","    for i in answerVectors:\n","      correct = 0\n","      for j in range(10000):\n","        if (i[j] == y_test[j]).all():\n","          correct += 1\n","      accuracy.append(correct)\n","\n","    epochAccuracy.append(accuracy)\n","  epochAccuracy = np.array(epochAccuracy)\n","  accuracies.append(np.max(epochAccuracy, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sxFPMVg5_rR"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxofJLmWt_q9"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFzMJh7er_2F"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm8zD52QuPZl"},"source":["## 12 Bits"]},{"cell_type":"code","metadata":{"id":"ZILL0GpC62ed"},"source":["accuracies = []\n","for intBits in range(13):\n","  epochAccuracy = []\n","  for epoch in range(5, 10):\n","    qmodel = modelMaker(13, intBits, epoch)\n","    answerVectors = []\n","    for iter in [2,3,4, 5, 6, 7, 8]:\n","      answerVectors.append(epicGeneratePredictions([0, 10000], 13, intBits, iter))\n","      #print (\"Done\")\n","\n","    accuracy = []\n","    y_test = get_one_hot(test_labels[0:10000], 10)\n","    for i in answerVectors:\n","      correct = 0\n","      for j in range(10000):\n","        if (i[j] == y_test[j]).all():\n","          correct += 1\n","      accuracy.append(correct)\n","\n","    epochAccuracy.append(accuracy)\n","  epochAccuracy = np.array(epochAccuracy)\n","  accuracies.append(np.max(epochAccuracy, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WExOpevEBpVP"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2w2rwUquO5h"},"source":["accuracies = []\n","for intBits in range(13):\n","  qmodel = modelMaker(13, intBits, 20)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 13, intBits, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  accuracies.append(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpa-4Efnu_64"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gU5FdCUBvQIW"},"source":["## 16 Bit"]},{"cell_type":"code","metadata":{"id":"bo-f16b3C4n6"},"source":["accuracies = []\n","for intBits in range(17):\n","  epochAccuracy = []\n","  for epoch in range(10, 18):\n","    qmodel = modelMaker(17, intBits, epoch)\n","    answerVectors = []\n","    for iter in [2,3,4, 5, 6, 7, 8]:\n","      answerVectors.append(epicGeneratePredictions([0, 10000], 17, intBits, iter))\n","      #print (\"Done\")\n","\n","    accuracy = []\n","    y_test = get_one_hot(test_labels[0:10000], 10)\n","    for i in answerVectors:\n","      correct = 0\n","      for j in range(10000):\n","        if (i[j] == y_test[j]).all():\n","          correct += 1\n","      accuracy.append(correct)\n","\n","    epochAccuracy.append(accuracy)\n","  epochAccuracy = np.array(epochAccuracy)\n","  accuracies.append(np.max(epochAccuracy, axis=0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKLSAwZ6PLnt"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iskmHQqZDa6n"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPIJLRxQQHp_"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rcnpGAOvPCA"},"source":["accuracies = []\n","for intBits in range(17):\n","  qmodel = modelMaker(17, intBits, 25)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 17, intBits, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  accuracies.append(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uotOwF73xipk"},"source":["accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"abJF8n-hwNXH"},"source":["accuracies "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ugo7ieT-zIwK"},"source":["# 24 bit accuracy values"]},{"cell_type":"markdown","metadata":{"id":"m3uqN5t9zPwI"},"source":["## Tensorflow"]},{"cell_type":"code","metadata":{"id":"fD-2DitlzRdc"},"source":["### Evaluation in a loop\n","precision = 25\n","intBits = 1\n","histories = []\n","mymodels = []\n","for epoch in range(3, 13):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c1'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c2'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='sigmoid',\n","        name='c3'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(Activation('sigmoid'))\n","  model.add(QActivation('quantized_bits({}, {})'.format(precision, intBits)))\n","  model.add(layers.Dense(10))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= epoch, \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  print (\"Done\")\n","\n","  model.evaluate(test_images, test_labels)\n","\n","  mymodels.append(model)\n","  histories.append(history)"],"execution_count":null,"outputs":[]}]}