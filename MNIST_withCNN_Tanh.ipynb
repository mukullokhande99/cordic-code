{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_withCNN_Tanh.ipynb","provenance":[{"file_id":"1Ovw6CtuYDS482zYzu9cZdbmSgyqRFAla","timestamp":1616242171871},{"file_id":"1sz3a1vF_7vcUEte6iG0o__MyVlMKCp7S","timestamp":1615879820365}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HxJYuaOPAmPv"},"source":["import multiprocessing \n","\n","import six\n","import numpy as np\n","import tensorflow.compat.v2 as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","!pip install git+https://github.com/google/qkeras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELP-dOZIWd1s"},"source":["# Tensorflow Colab file with some modifications"]},{"cell_type":"code","metadata":{"id":"6du4sCo7Wifx"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","\n","from qkeras import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cat1JGJYWnY8"},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","def get_one_hot(targets, nb_classes):\n","    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n","    return res.reshape(list(targets.shape)+[nb_classes])\n","#train_labels, test_labels = get_one_hot(train_labels, 10), get_one_hot(test_labels, 10)#\n","\n","train_images = train_images.reshape(train_images.shape + (1,)).astype(\"float32\")\n","test_images = test_images.reshape(test_images.shape + (1,)).astype(\"float32\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fhu2SVYtljof"},"source":["def get_data():\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n","    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n","\n","    x_train /= 256.0\n","    x_test /= 256.0\n","\n","    x_mean = np.mean(x_train, axis=0)\n","\n","    x_train -= x_mean\n","    x_test -= x_mean\n","\n","    nb_classes = np.max(y_train)+1\n","    y_train = to_categorical(y_train, nb_classes)\n","    y_test = to_categorical(y_test, nb_classes)\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","(train_images, train_labels), (test_images, test_labels) = get_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaEdKdcGojSp"},"source":["np.mean(test_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-C9Zc2lWqx6"},"source":["intBits = 1\n","precision = 16\n","\n","from tensorflow.keras.optimizers import Adam\n","\n","model = models.Sequential()\n","#model.add(layers.Conv2D(32, (3,3), activation='tanh'))\n","model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c1'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","#model.add(layers.Conv2D(64, (3,3), activation='tanh'))\n","model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c2'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","#model.add(layers.Conv2D(32, (3,3), activation='tanh'))\n","model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","model.add(layers.Flatten())\n","\n","#model.add(layers.Dense(64, activation='tanh'))\n","#model.add(layers.Dense(10))\n","model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d2'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJfLC9yrWu3Z"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs=7, \n","                    validation_data=(test_images[:5000], test_labels[:5000]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw5LYwqdbR54"},"source":["import qkeras\n","from qkeras import *\n","from qkeras.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86ytLEYSbZ0e"},"source":["model_save_quantized_weights(model)\n","print (\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x89GmBLzbfXZ"},"source":["model.evaluate(test_images[5000:], test_labels[5000:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgG4J9qoeE3n"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","\n","for precision in [8, 12, 16, 32]:\n","  for epoch in range(3, 13):\n","    model = models.Sequential()\n","    #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","    model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c1'))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","    model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c2'))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","    model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c3'))\n","\n","    model.add(layers.Flatten())\n","    #model.add(layers.Dense(64, activation='relu'))\n","    model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          name='d1'))\n","    model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","    model.add(layers.Dense(10))\n","\n","    model.compile(optimizer='adam',\n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","\n","    history = model.fit(train_images[:48000], train_labels[:48000], epochs= epoch, \n","                      validation_data=(train_images[48000:60000], train_labels[48000:60000]), verbose=False)\n","  \n","    model_save_quantized_weights(model)\n","    print (\"Done\")\n","\n","    model.evaluate(test_images, test_labels)\n","\n","    mymodels.append(model)\n","    histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcTO8AQxCVTT"},"source":["mymodels[3].layers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-fHgcFxBkWS"},"source":["# Now we go for CORDIC "]},{"cell_type":"code","metadata":{"id":"gJX3XfYHBn_q"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  #xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  #yarr = np.zeros(shape = (len(arr), len(arr[0])))\n","\n","  xarr = 1.2075*np.ones(shape=arr.shape)\n","  yarr = np.zeros(shape = arr.shape)\n","\n","  global  lookup_arctanh\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","    \n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","  return quantized_bits(precision, 1, alpha=1)(yarr / xarr)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rakvv3FSPdqa"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(x, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  \n","\n","  global  lookup_arctanh\n","  current_vector = np.array([1.2075, 0])\n","  z = x\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(z)\n","    x = current_vector[0]\n","    y = current_vector[1]\n","    xnew = x\n","    ynew = y\n","    xnew = xnew - m*sigma*2**(-i)*y\n","    ynew = ynew + sigma*2**(-i)*x\n","    z = z - sigma*lookup_arctanh[i-1]\n","    current_vector = [xnew, ynew]\n","\n","  \n","  ex = current_vector[0] + current_vector[1]\n","  eminusx = current_vector[0] - current_vector[1]\n","\n","  temp1 = quantized_bits(precision,1,alpha=1)(ex - eminusx)\n","  temp2 = quantized_bits(precision, 1, alpha=1)(ex + eminusx)\n","\n","  return quantized_bits(precision,1, alpha=1)(temp1/temp2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t658NEF4Wi-y"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(x, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  \n","  x = 2*x\n","  global  lookup_arctanh\n","  current_vector = np.array([1.2075, 0])\n","  z = x\n","  \n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(z)\n","    x = current_vector[0]\n","    y = current_vector[1]\n","    xnew = x\n","    ynew = y\n","    xnew = xnew - m*sigma*2**(-i)*y\n","    ynew = ynew + sigma*2**(-i)*x\n","    z = z - sigma*lookup_arctanh[i-1]\n","    current_vector = [xnew, ynew]\n","\n","  \n","  ex = current_vector[0] + current_vector[1]\n","  eminusx = current_vector[0] - current_vector[1]\n","\n","  temp1 = quantized_bits(precision,1,alpha=1)(ex - 1)\n","  temp2 = quantized_bits(precision, 1, alpha=1)(ex + 1)\n","\n","  return quantized_bits(precision,1, alpha=1)(temp1/temp2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaLCBbclbsWL"},"source":["tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n","lookup_arctanh = np.arctanh(tangents)\n","\n","def modifiedCordicTanh(arr, precision, iterations):\n","  '''\n","  Returns the quantized tanh of the supplied argument x\n","  '''\n","  \n","\n","  global  lookup_arctanh\n","  current_vector = np.array([1.2075, 0])\n","  arr *= 2\n","\n","  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n","  yarr = np.zeros(shape=(len(arr), len(arr[0])))\n","\n","  for i in range(1, iterations+1):\n","    m = -1\n","    sigma = np.sign(arr)\n","\n","    xchange = m*sigma*2**(-i)*yarr\n","    ychange = sigma*2**(-i)*xarr\n","    arrchange = sigma*lookup_arctanh[i-1]\n","\n","    xarr -= xchange\n","    yarr += ychange\n","    arr -= arrchange\n","    #x = current_vector[0]\n","    #y = current_vector[1]\n","    #xnew = x\n","    #ynew = y\n","    #xnew = xnew - m*sigma*2**(-i)*y\n","    #ynew = ynew + sigma*2**(-i)*x\n","    #z = z - sigma*lookup_arctanh[i-1]\n","    #current_vector = [xnew, ynew]\n","\n","  return quantized_bits(precision, 1, alpha=1)(2/(1 + xarr - yarr) - 1)\n","\n","  #eminus2x = xarr - yarr\n","  #temp1 = quantized_bits(9, 1, alpha=1)(1 + eminus2x)\n","  #temp2 = quantized_bits(9, 1, alpha=1)(1/temp1)\n","  #temp3 = quantized_bits(9, 1, alpha=1)(2*temp2)\n","  #return quantized_bits(9, 1, alpha=1)(temp3 - 1)\n","\n","  #eminus2x = current_vector[0] - current_vector[1]\n","  #temp1 = quantized_bits(9, 1, alpha=1)(1 + eminus2x)\n","  #temp2 = quantized_bits(9, 1, alpha=1)(1/ temp1)\n","  #temp3 = quantized_bits(9, 1, alpha = 1)(2*temp2)\n","  \n","  #return quantized_bits(9,1, alpha=1)(temp3 - 1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5gu-8WLCAXv"},"source":["qmodel = 0 \n","\n","\n","def epicGeneratePredictions(indices, precision, iterations):\n","  global qmodel\n","  x_test = test_images\n","  from keras import backend as K\n","  get_sixth_layer_output = K.function([qmodel.layers[0].input],\n","                                    [qmodel.layers[6].output])\n"," \n","  layer6_output = get_sixth_layer_output([x_test])[0]\n","\n","  layer6_output = layer6_output[indices[0]: indices[1]]\n","\n","  layer7_output = modifiedCordicTanh(layer6_output, precision, iterations)\n","\n","  input_shape = qmodel.layers[8].get_input_shape_at(0)\n","  layer_input = Input(shape=(64))\n","  x = layer_input\n","  x = qmodel.layers[8](x)\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(layer7_output))\n","\n","  #predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  a = predictions\n","  return (a == a.max(axis=1)[:,None]).astype(int)\n","\"\"\"\n","\n","def epicGeneratePredictions(indices, precision, iterations):\n","  global qmodel\n","  x_test = test_images\n","  from keras import backend as K\n","\n","  #This generates output of 1st Conv2D layer\n","  get_zeroth_layer_output = K.function([qmodel.layers[0].input],\n","                                    [qmodel.layers[0].output])\n","  \n","  layer0_output = get_zeroth_layer_output([x_test])[0]\n","\n","  layer0_output = layer0_output[indices[0]: indices[1]]\n","\n","  #print (layer0_output.shape)\n","  #This generates output of first QActivation\n","  layer1_output = modifiedCordicTanh(layer0_output, precision, iterations)\n","\n","  # This generates output of 2nd Conv2D layer\n","  input_shape = qmodel.layers[2].get_input_shape_at(0)\n","  layer_input = Input(input_shape)\n","  x = layer_input\n","  x = qmodel.layers[3](qmodel.layers[2](x))\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(layer1_output))\n","  del layer1_output\n","  del layer0_output\n","  del get_zeroth_layer_output\n","  #This generates output of 2nd QActivation layer\n","  predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  #This generates output of the 3rd QConv2D layer\n","  input_shape = qmodel.layers[5].get_input_shape_at(0)\n","  layer_input = Input(input_shape)\n","  x = layer_input\n","  x = qmodel.layers[6](qmodel.layers[5](x))\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(predictions))\n","\n","  #This generates output of 3rd Qactivation layer\n","  predictions = modifiedCordicTanh(predictions, precision, iterations)\n","\n","  #This generates output of 1st QDense layer\n","  input_shape = qmodel.layers[8].get_input_shape_at(0)\n","  layer_input = Input(input_shape)\n","  x = layer_input\n","  x = qmodel.layers[9](qmodel.layers[8](x))\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(predictions))\n","\n","  #This generates output of 1st Dense QActivation\n","  predictions = np.array(qm4.predict(predictions))\n","\n","  #This generates the last layer's output\n","  input_shape = qmodel.layers[11].get_input_shape_at(0)\n","  layer_input = Input(input_shape)\n","  x = layer_input\n","  x = qmodel.layers[11](x)\n","  qm4 = Model(layer_input, x)\n","\n","  predictions = np.array(qm4.predict(predictions))\n","  del q4\n","  del x\n","  del get_input_shape\n","  del layer_input\n","\n","  a = predictions\n","  return (a == a.max(axis=1)[:,None]).astype(int)\n","  \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxSrKO_fFqgc"},"source":["\n","def modelMaker(precision, intBits, epoch=10):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c1'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c2'))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        activation='quantized_tanh({}, {})'.format(precision, intBits),\n","        name='c3'))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","  model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d2'))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              #loss = \"categorical_crossentropy\",\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= epoch, \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  return model\n","\n","\"\"\"\n","def modelMaker(precision, intBits):\n","  model = models.Sequential()\n","  #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","  model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='c1'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='c2'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","\n","  model.add(layers.MaxPooling2D((2, 2)))\n","  #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","  model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='c3'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","\n","  model.add(layers.Flatten())\n","  #model.add(layers.Dense(64, activation='relu'))\n","  model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d1'))\n","  model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","  model.add(QDense(10, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","        name='d2'))\n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              #loss = \"categorical_crossentropy\",\n","              metrics=['accuracy'])\n","\n","  history = model.fit(train_images[:48000], train_labels[:48000], epochs= 5 + int(3*(precision/4 - 2)), \n","                    validation_data=(train_images[48000:], train_labels[48000:]), verbose=False)\n","  \n","  model_save_quantized_weights(model)\n","  return model\n","  \"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u65j3YvrEtc5"},"source":["## 8bit"]},{"cell_type":"code","metadata":{"id":"ShUnffV_I6uV"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(9, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 9,iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_p-hbBHQBr4"},"source":["# ex - e-x / ex + e-x\n","\n","for epoch in range(3, 15):\n","  qmodel = modelMaker(9, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 9,iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9XZ9BXgWWKv"},"source":["# e2x - 1/ e2x + 1\n","\n","for epoch in range(3, 15):\n","  qmodel = modelMaker(9, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 9,iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kd4nmWaNb12q"},"source":["#2sig2x - 1\n","\n","for epoch in range(3, 15):\n","  qmodel = modelMaker(9, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 9,iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0LgZum2EqgJ"},"source":["qmodel = modelMaker(9, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0,10000], 9, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","#y_test = test_labels[0:10000]\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amUfh8P79wDj"},"source":["qmodel.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpWPjkxLmp0P"},"source":["answerVectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wE6v7ipJNtZk"},"source":["accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","#y_test = test_labels[0:10000]\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqwi29lx7qvp"},"source":["y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQrjTsGWtXZJ"},"source":["## 12 bit"]},{"cell_type":"code","metadata":{"id":"lnOQGTRyMwre"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(13, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 13, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9-bsjjctZy0"},"source":["qmodel = modelMaker(13, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 13, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wLpXMAkuQ1D"},"source":["## 16 bit"]},{"cell_type":"code","metadata":{"id":"QHOvPR6GP9PP"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(17, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 17, iter))\n","    #print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9YhLNeHuNWB"},"source":["qmodel = modelMaker(17, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 17, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ur-znNAwvA8h"},"source":["## 24 bit"]},{"cell_type":"code","metadata":{"id":"PXKt6zNgvDMz"},"source":["for epoch in range(3, 15):\n","  qmodel = modelMaker(25, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 25, iter))\n","    print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3PbNMoJr84a"},"source":["for epoch in range(14, 20):\n","  qmodel = modelMaker(25, 1, epoch)\n","  answerVectors = []\n","  for iter in [2,3,4, 5, 6, 7, 8]:\n","    answerVectors.append(epicGeneratePredictions([0, 10000], 25, iter))\n","    print (\"Done\")\n","\n","  accuracy = []\n","  y_test = get_one_hot(test_labels[0:10000], 10)\n","  for i in answerVectors:\n","    correct = 0\n","    for j in range(10000):\n","      if (i[j] == y_test[j]).all():\n","        correct += 1\n","    accuracy.append(correct)\n","\n","  print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSedv_upzjW6"},"source":["## 32 Bit"]},{"cell_type":"code","metadata":{"id":"t1QD3sZ0zh1L"},"source":["qmodel = modelMaker(33, 1)\n","answerVectors = []\n","for iter in [2,3,4, 5, 6, 7, 8]:\n","  answerVectors.append(epicGeneratePredictions([0, 10000], 33, iter))\n","  print (\"Done\")\n","\n","accuracy = []\n","y_test = get_one_hot(test_labels[0:10000], 10)\n","for i in answerVectors:\n","  correct = 0\n","  for j in range(10000):\n","    if (i[j] == y_test[j]).all():\n","      correct += 1\n","  accuracy.append(correct)\n","\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qkKXdo5mmWEG"},"source":["# for 24 bit only"]},{"cell_type":"markdown","metadata":{"id":"Q-WlQ2PbmZEB"},"source":["## Tensorflow"]},{"cell_type":"code","metadata":{"id":"NxFP6X_Ema0V"},"source":["### Evaluation in a loop\n","intBits = 1\n","histories = []\n","mymodels = []\n","\n","for precision in [24]:\n","  for epoch in range(3, 15):\n","    model = models.Sequential()\n","    #model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n","    model.add(QConv2D(32, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c1'))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","    model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c2'))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","    model.add(QConv2D(64, (3,3), kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          activation='quantized_tanh({}, {})'.format(precision, intBits),\n","          name='c3'))\n","\n","    model.add(layers.Flatten())\n","    #model.add(layers.Dense(64, activation='relu'))\n","    model.add(QDense(64, kernel_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          bias_quantizer=\"quantized_bits({}, {} , alpha=1)\".format(precision, intBits),\n","          name='d1'))\n","    model.add(QActivation('quantized_tanh({}, {})'.format(precision, intBits)))\n","    model.add(layers.Dense(10))\n","\n","    model.compile(optimizer='adam',\n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","\n","    history = model.fit(train_images[:48000], train_labels[:48000], epochs= epoch, \n","                      validation_data=(train_images[48000:60000], train_labels[48000:60000]), verbose=False)\n","  \n","    model_save_quantized_weights(model)\n","    print (\"Done\")\n","\n","    model.evaluate(test_images, test_labels)\n","\n","    mymodels.append(model)\n","    histories.append(history)"],"execution_count":null,"outputs":[]}]}