{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Revised_BitPrecisionAccuracy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLhXJPmaRmhW",
        "outputId": "76ef8aec-4a64-41e6-bf92-cc707e3c6b20"
      },
      "source": [
        " import six\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install git+https://github.com/google/qkeras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google/qkeras\n",
            "  Cloning https://github.com/google/qkeras to /tmp/pip-req-build-7a81044c\n",
            "  Running command git clone -q https://github.com/google/qkeras /tmp/pip-req-build-7a81044c\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.19.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.4.1)\n",
            "Collecting pyparser\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/7c/77a059dcf29b39e6c4315ce9e9c7d4959be6f13af8ee42e6d5376b599015/pyparser-1.0.tar.gz\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (50.3.2)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (2.5)\n",
            "Collecting keras-tuner>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 15.2MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/54/115f0c28a61d56674c3a5e05c46d6c3523ad196e1dcd3e2d8b119026df36/tqdm-4.54.1-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hCollecting parse==1.6.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/eb/44c70a5704fdf55d26a33070a9a13a03f0d66a5f6b72cadf75620d9dc4c0/parse-1.6.5.tar.gz\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (1.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (0.1.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->QKeras==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (20.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.23.1->QKeras==0.8.0) (1.0.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner>=1.0.1->QKeras==0.8.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (3.0.4)\n",
            "Building wheels for collected packages: QKeras, pyparser, keras-tuner, parse, terminaltables\n",
            "  Building wheel for QKeras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for QKeras: filename=QKeras-0.8.0-cp36-none-any.whl size=143338 sha256=40a2ac3891a1652696557c0cb560dbf83fcc74f64d2bcf1adad7904708e8c1de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vw44z8mu/wheels/b4/74/1d/9456d62789716894a5edd7e342b4beaef69241ac584706c68d\n",
            "  Building wheel for pyparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyparser: filename=pyparser-1.0-cp36-none-any.whl size=4944 sha256=b1b8972692f208a5c1245eb4f3b6b106f34d9688d7d114773fb98aa8708b08b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/1c/4f/9f66cd69719aa41c2684efae758d95db7803e9fe1f65146db1\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=c34b71e95470fa13172a3aefbd00206f4bbe426f9c13d0cd370019cb4b60432e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.6.5-cp36-none-any.whl size=18178 sha256=18878c6c48b2f626615ce942a55aa6ff62b9562b19c579a83073ca2eba64908a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/cd/20/b1fc8e3046811c0d10f03d1027c26077c9ca396cf8d579f4e3\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=79301ea6b2370545737eba9dbb8391f290a8a5d38d7b54c518ee706f5bb24922\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built QKeras pyparser keras-tuner parse terminaltables\n",
            "Installing collected packages: parse, pyparser, tensorflow-model-optimization, terminaltables, colorama, tqdm, threadpoolctl, scikit-learn, keras-tuner, QKeras\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed QKeras-0.8.0 colorama-0.4.4 keras-tuner-1.0.2 parse-1.6.5 pyparser-1.0 scikit-learn-0.23.2 tensorflow-model-optimization-0.5.0 terminaltables-3.1.0 threadpoolctl-2.1.0 tqdm-4.54.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htVC-VWkSRsE"
      },
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "def CreateQModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_1\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_2\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_3\")(x)\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwEvKjA9SsHb",
        "outputId": "05ebcef7-c151-4faf-f366-25a977c5e37c"
      },
      "source": [
        "def get_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
        "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
        "\n",
        "    x_train /= 256.0\n",
        "    x_test /= 256.0\n",
        "\n",
        "    x_mean = np.mean(x_train, axis=0)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_test -= x_mean\n",
        "\n",
        "    nb_classes = np.max(y_train)+1\n",
        "    y_train = to_categorical(y_train, nb_classes)\n",
        "    y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZmh7Zl6W4bk"
      },
      "source": [
        "from qkeras.utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgwsFAZ9To9o",
        "outputId": "f39b9d6e-dcc4-419c-a947-9b642454f8f0"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "histories = []\n",
        "qmodels = []\n",
        "for i in range(0, 9):\n",
        "  qmodel = CreateQModel(x_train.shape[1:], y_train.shape[-1], i)\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "  qmodel.compile(\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      optimizer=Adam(0.0005),\n",
        "      metrics=[\"accuracy\"])\n",
        "  history = qmodel.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False,callbacks=[callback])\n",
        "  model_save_quantized_weights(qmodel)\n",
        "\n",
        "  qmodels.append(qmodel)\n",
        "  histories.append(history)\n",
        "  print (\"Done with model with num of integer bits = {}\".format(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... quantizing model\n",
            "Done with model with num of integer bits = 0\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 1\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 2\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 3\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 4\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 5\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 6\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 7\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMJyxWr7WHzh",
        "outputId": "af3ead42-23f4-4434-9b75-ab0790e0d854"
      },
      "source": [
        "for i in range(9):\n",
        "  qmodels[i].evaluate(x=x_test[5000:], y=y_test[5000:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9840\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9840\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9806\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9476\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9450\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9400\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 2.3466 - accuracy: 0.9434\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 27.1492 - accuracy: 0.8724\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 3867.5432 - accuracy: 0.6878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1qBrPjkpqqK"
      },
      "source": [
        "# Now for 12 bits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk9h6WimpuXe"
      },
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "def CreateQModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(13, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "    x = QActivation(\"quantized_tanh(13, {})\".format(intBits), name=\"act_1\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "    x = QActivation(\"quantized_tanh(13, {})\".format(intBits), name=\"act_2\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "    x = QActivation(\"quantized_tanh(13, {})\".format(intBits), name=\"act_3\")(x)\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(13, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4jfGWUipy_A",
        "outputId": "4f10d79f-028b-45a4-fe0a-fc4a3b096124"
      },
      "source": [
        "from qkeras.utils import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "histories2 = []\n",
        "qmodels2 = []\n",
        "for i in range(0, 13):\n",
        "  qmodel = CreateQModel(x_train.shape[1:], y_train.shape[-1], i)\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "  qmodel.compile(\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      optimizer=Adam(0.0005),\n",
        "      metrics=[\"accuracy\"])\n",
        "  history = qmodel.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False,callbacks=[callback])\n",
        "  model_save_quantized_weights(qmodel)\n",
        "\n",
        "  qmodels2.append(qmodel)\n",
        "  histories2.append(history)\n",
        "  print (\"Done with model with num of integer bits = {}\".format(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... quantizing model\n",
            "Done with model with num of integer bits = 0\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 1\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 2\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 3\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 4\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 5\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 6\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 7\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 8\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 9\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 10\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 11\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0d9haroqFlw",
        "outputId": "080b5318-011a-45c7-c93c-b46170569571"
      },
      "source": [
        "for i in range(13):\n",
        "  qmodels2[i].evaluate(x=x_test[5000:], y=y_test[5000:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9828\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9830\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9810\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9404\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2086 - accuracy: 0.9434\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9458\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9444\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9434\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9432\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9374\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 15.6745 - accuracy: 0.8778\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 494.9062 - accuracy: 0.8598\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 806845.2500 - accuracy: 0.3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLcZ1A7AtkgH"
      },
      "source": [
        "# Now for 16 bits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD_woEIBtoSi"
      },
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "def CreateQModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(17, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "    x = QActivation(\"quantized_tanh(17, {})\".format(intBits), name=\"act_1\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "    x = QActivation(\"quantized_tanh(17, {})\".format(intBits), name=\"act_2\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "    x = QActivation(\"quantized_tanh(17, {})\".format(intBits), name=\"act_3\")(x)\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(17, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icha6ua5tzWE",
        "outputId": "850dbbfc-977f-4c8b-f541-5c82972cef83"
      },
      "source": [
        "from qkeras.utils import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "histories3 = []\n",
        "qmodels3 = []\n",
        "for i in range(0, 17):\n",
        "  qmodel = CreateQModel(x_train.shape[1:], y_train.shape[-1], i)\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "  qmodel.compile(\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      optimizer=Adam(0.0005),\n",
        "      metrics=[\"accuracy\"])\n",
        "  history = qmodel.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False,callbacks=[callback])\n",
        "  model_save_quantized_weights(qmodel)\n",
        "\n",
        "  qmodels3.append(qmodel)\n",
        "  histories3.append(history)\n",
        "  print (\"Done with model with num of integer bits = {}\".format(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... quantizing model\n",
            "Done with model with num of integer bits = 0\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 1\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 2\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 3\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 4\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 5\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 6\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 7\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 8\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 9\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 10\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 11\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 12\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 13\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 14\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 15\n",
            "... quantizing model\n",
            "Done with model with num of integer bits = 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK40zA0bt87V",
        "outputId": "4d1d9e73-7cda-4091-b121-62dcef896125"
      },
      "source": [
        "for i in range(17):\n",
        "  qmodels3[i].evaluate(x=x_test[5000:], y=y_test[5000:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9826\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9824\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9848\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9382\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9420\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9446\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9442\n",
            "157/157 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9438\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2158 - accuracy: 0.9402\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9438\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9442\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9464\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9408\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9400\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 59.1526 - accuracy: 0.8858\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 8766.6611 - accuracy: 0.8260\n",
            "157/157 [==============================] - 0s 2ms/step - loss: 91481448.0000 - accuracy: 0.1190\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}