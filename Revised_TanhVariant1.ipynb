{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Revised_TanhVariant1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LjBaGTpbkcC"
      },
      "source": [
        "Here we evaluate the accuracy for Tanh activation function with the expression:\n",
        "\n",
        "Tanh = sinh / cosh \n",
        "\n",
        "Accuracies:\n",
        "\n",
        "97, 97.8, 97.7, 97.9, 97.9, 97.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NmuD5nJ2Ne9",
        "outputId": "1d737741-62e2-4651-de9f-c998e3d4a98d"
      },
      "source": [
        "import multiprocessing \n",
        "\n",
        "import six\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install git+https://github.com/google/qkeras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google/qkeras\n",
            "  Cloning https://github.com/google/qkeras to /tmp/pip-req-build-341taflq\n",
            "  Running command git clone -q https://github.com/google/qkeras /tmp/pip-req-build-341taflq\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.19.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.4.1)\n",
            "Collecting pyparser\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/7c/77a059dcf29b39e6c4315ce9e9c7d4959be6f13af8ee42e6d5376b599015/pyparser-1.0.tar.gz\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (50.3.2)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (2.5)\n",
            "Collecting keras-tuner>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 23.9MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/54/115f0c28a61d56674c3a5e05c46d6c3523ad196e1dcd3e2d8b119026df36/tqdm-4.54.1-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.6MB/s \n",
            "\u001b[?25hCollecting parse==1.6.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/eb/44c70a5704fdf55d26a33070a9a13a03f0d66a5f6b72cadf75620d9dc4c0/parse-1.6.5.tar.gz\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (0.1.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->QKeras==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (20.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (2.23.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.23.1->QKeras==0.8.0) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner>=1.0.1->QKeras==0.8.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2020.12.5)\n",
            "Building wheels for collected packages: QKeras, pyparser, keras-tuner, parse, terminaltables\n",
            "  Building wheel for QKeras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for QKeras: filename=QKeras-0.8.0-cp36-none-any.whl size=143338 sha256=411ddad962899cd628f88d1fd1769da5a15a42fc9d936c562067c48b11a6b421\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7jwgfk0h/wheels/b4/74/1d/9456d62789716894a5edd7e342b4beaef69241ac584706c68d\n",
            "  Building wheel for pyparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyparser: filename=pyparser-1.0-cp36-none-any.whl size=4944 sha256=3319bcf0cc0d333451b1cf9695791887f12b575b1e8546a2b7a2130dd8ce6e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/1c/4f/9f66cd69719aa41c2684efae758d95db7803e9fe1f65146db1\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=801aa21791b28cf1280d47b622548d7ca2a9252af0660c45eba3e85cfef888f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.6.5-cp36-none-any.whl size=18178 sha256=a9d40d09518d2974988476ebef3d601861cf55d2e97def7fbb7ad0de44111496\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/cd/20/b1fc8e3046811c0d10f03d1027c26077c9ca396cf8d579f4e3\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=45d9a02da6abf3285f28e41745c2b0f502c791be446e1fd2df622c94dea31a73\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built QKeras pyparser keras-tuner parse terminaltables\n",
            "Installing collected packages: parse, pyparser, tensorflow-model-optimization, terminaltables, colorama, tqdm, threadpoolctl, scikit-learn, keras-tuner, QKeras\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed QKeras-0.8.0 colorama-0.4.4 keras-tuner-1.0.2 parse-1.6.5 pyparser-1.0 scikit-learn-0.23.2 tensorflow-model-optimization-0.5.0 terminaltables-3.1.0 threadpoolctl-2.1.0 tqdm-4.54.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fCAtT9NbTw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ef0e33-c050-47c9-c795-dc6b2b49e258"
      },
      "source": [
        "def get_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
        "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
        "\n",
        "    x_train /= 256.0\n",
        "    x_test /= 256.0\n",
        "\n",
        "    x_mean = np.mean(x_train, axis=0)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_test -= x_mean\n",
        "\n",
        "    nb_classes = np.max(y_train)+1\n",
        "    y_train = to_categorical(y_train, nb_classes)\n",
        "    y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yDwMDsL20UP"
      },
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "def CreateQModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_1\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_2\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_3\")(x)\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dXvoEIW2_kS",
        "outputId": "8b045957-e5b1-4f6d-c6c9-c6ae53dbdbe4"
      },
      "source": [
        "from qkeras.utils import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "qmodel = CreateQModel(x_train.shape[1:], y_train.shape[-1], 1)\n",
        "qmodel.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=Adam(0.0005),\n",
        "    metrics=[\"accuracy\"])\n",
        "history = qmodel.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False, callbacks=[callback])\n",
        "model_save_quantized_weights(qmodel)\n",
        "print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... quantizing model\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM-WtFnp4elG",
        "outputId": "d80f3ba8-ff28-4fbd-9853-f155c8019ac7"
      },
      "source": [
        "qmodel.evaluate(x_test[5000:], y_test[5000:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.056499872356653214, 0.9847999811172485]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q2tGtQW4vVq"
      },
      "source": [
        "tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n",
        "lookup_arctanh = np.arctanh(tangents)\n",
        "\n",
        "def cordicTanh(x, iterations):\n",
        "  '''\n",
        "  Returns the quantized tanh of the supplied argument x\n",
        "  '''\n",
        "  \n",
        "\n",
        "  global  lookup_arctanh\n",
        "  current_vector = np.array([1.2075, 0])\n",
        "  z = x\n",
        "\n",
        "  for i in range(1, iterations+1):\n",
        "    m = -1\n",
        "    sigma = np.sign(z)\n",
        "    x = current_vector[0]\n",
        "    y = current_vector[1]\n",
        "    xnew = x\n",
        "    ynew = y\n",
        "    xnew = xnew - m*sigma*2**(-i)*y\n",
        "    ynew = ynew + sigma*2**(-i)*x\n",
        "    z = z - sigma*lookup_arctanh[i-1]\n",
        "    current_vector = [xnew, ynew]\n",
        "\n",
        "  \n",
        "  return quantized_bits(9,1, alpha=1)(current_vector[1]/current_vector[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIcVEBBk5Yvv"
      },
      "source": [
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "justAHolder = 0\n",
        "numOfIterations = 0\n",
        "\n",
        "def calcAndStore(argument):\n",
        "  global justAHolder\n",
        "  i = argument[0]\n",
        "  j = argument[1]\n",
        "  justAHolder[i][j] = cordicTanh(justAHolder[i][j], numOfIterations)\n",
        "\n",
        "def doit(arguments):\n",
        "  if __name__ == '__main__':\n",
        "    pool = ThreadPool(13)\n",
        "    pool.map(calcAndStore, arguments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D06EHyTA6DVD"
      },
      "source": [
        "x_test = x_test[5000:]\n",
        "y_test = y_test[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mbbqqjh5aWN"
      },
      "source": [
        "def epicGeneratePredictions(indices, iterations):\n",
        "  global justAHolder, numOfIterations\n",
        "  from keras import backend as K\n",
        "  get_third_layer_output = K.function([qmodel.layers[0].input],\n",
        "                                    [qmodel.layers[2].output])\n",
        " \n",
        "  layer3_output = get_third_layer_output([x_test])[0]\n",
        "\n",
        "  layer3_output = layer3_output[indices[0]: indices[1]]\n",
        "\n",
        "  ### Let us try and sidestep this ###\n",
        "  #for i in range(len(layer3_output)):\n",
        "  #  for j in range(len(layer3_output[0])):\n",
        "  #    layer3_output[i][j] = cordicSigmoid(layer3_output[i][j], iterations)\n",
        "\n",
        "  justAHolder = layer3_output\n",
        "  numOfIterations = iterations\n",
        "  arguments = np.dstack(np.meshgrid(np.array(range(len(layer3_output))), range(len(layer3_output[0])))).reshape(-1, 2)\n",
        "  doit(arguments)\n",
        "\n",
        "  layer3_output = justAHolder\n",
        "\n",
        "  print (\"Done!!!\")\n",
        "\n",
        "  input_shape = qmodel.layers[4].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(256))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[4](x)\n",
        "  qm4 = Model(layer_input, x)\n",
        "\n",
        "  predictions = qm4.predict(layer3_output)\n",
        "\n",
        "\n",
        "  justAHolder = predictions\n",
        "  numOfIterations = iterations\n",
        "  arguments = np.dstack(np.meshgrid(np.array(range(len(predictions))), range(len(predictions[0])))).reshape(-1, 2)\n",
        "  doit(arguments)\n",
        "\n",
        "  predictions = justAHolder\n",
        "\n",
        "  #for i in range(len(predictions)):\n",
        "  #  for j in range(len(predictions[0])):\n",
        "  #    predictions[i][j] = cordicSigmoid(predictions[i][j], iterations)\n",
        "\n",
        "  input_shape = qmodel.layers[6].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(128))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[6](x)\n",
        "  qm4 = Model(layer_input, x)\n",
        "\n",
        "  predictions = qm4.predict(predictions)\n",
        "\n",
        "\n",
        "  justAHolder = predictions\n",
        "  numOfIterations = iterations\n",
        "  arguments = np.dstack(np.meshgrid(np.array(range(len(predictions))), range(len(predictions[0])))).reshape(-1, 2)\n",
        "  doit(arguments)\n",
        "\n",
        "  predictions = justAHolder\n",
        "\n",
        "  #for i in range(len(predictions)):\n",
        "  #  for j in range(len(predictions[0])):\n",
        "  #    predictions[i][j] = cordicSigmoid(predictions[i][j], iterations)\n",
        "\n",
        "  input_shape = qmodel.layers[8].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(128))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[8](x)\n",
        "\n",
        "  qm4 = Model(layer_input, x)\n",
        "  predictions = np.array(qm4.predict(predictions))\n",
        "\n",
        "  #answer = np.zeros_like(predictions)\n",
        "  #answer[len(predictions), predictions.argmax()] = 1\n",
        "  a = predictions\n",
        "  return (a == a.max(axis=1)[:,None]).astype(int)\n",
        "  #return answer\n",
        "\n",
        "  #return answer\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQmyyElE5mbz",
        "outputId": "9b0b06e8-39f9-4ee0-c324-516175c5572a"
      },
      "source": [
        "answerVectors = []\n",
        "for iter in [2,3,4]:\n",
        "  answerVectors.append(epicGeneratePredictions([0, 1000], iter))\n",
        "  print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!!!\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa797cd9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Done\n",
            "Done!!!\n",
            "Done\n",
            "Done!!!\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sIjrlwbCj0E",
        "outputId": "bb68eb7a-cc1b-4983-d41c-464c9a360790"
      },
      "source": [
        "accuracy = []\n",
        "for i in answerVectors:\n",
        "  correct = 0\n",
        "  for j in range(1000):\n",
        "    if (i[j] == y_test[j]).all():\n",
        "      correct += 1\n",
        "  accuracy.append(correct)\n",
        "\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[970, 978, 977]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQQxYkQSClAt",
        "outputId": "e3d5f405-2d2e-41d6-90df-95591b883af1"
      },
      "source": [
        "answerVectors = []\n",
        "for iter in [5, 6, 7]:\n",
        "  answerVectors.append(epicGeneratePredictions([0, 1000], iter))\n",
        "  print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!!!\n",
            "Done\n",
            "Done!!!\n",
            "Done\n",
            "Done!!!\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNRNOrrICsRd",
        "outputId": "ffb6c4e2-51a2-423f-afe4-5512097931a6"
      },
      "source": [
        "accuracy = []\n",
        "for i in answerVectors:\n",
        "  correct = 0\n",
        "  for j in range(1000):\n",
        "    if (i[j] == y_test[j]).all():\n",
        "      correct += 1\n",
        "  accuracy.append(correct)\n",
        "\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[979, 979, 979]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0AyHbhs5xWT",
        "outputId": "1b319210-7b2e-4e28-a21f-45a4646c94b8"
      },
      "source": [
        "answerVectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzsOvaBI54Yd",
        "outputId": "2a36cba4-cb3a-44d7-e152-faf6486308e5"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}