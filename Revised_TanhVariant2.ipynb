{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Revised_TanhVariant2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ26nEtq70vC"
      },
      "source": [
        "Here, we evaluate the accuracy of the Tanh activation function using the expression\n",
        "\n",
        "tanh = ex - e(-x)/(ex + e(-x)\n",
        "\n",
        "Accuracies\n",
        "\n",
        "97.4, 98.2, 98.1, 98.1, 98.1, 98.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE0RNV1M7zd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b3d19e-b137-4947-d7d0-9051113ca697"
      },
      "source": [
        "import multiprocessing \n",
        "\n",
        "import six\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install git+https://github.com/google/qkeras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/google/qkeras\n",
            "  Cloning https://github.com/google/qkeras to /tmp/pip-req-build-oybfpal3\n",
            "  Running command git clone -q https://github.com/google/qkeras /tmp/pip-req-build-oybfpal3\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.19.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (1.4.1)\n",
            "Collecting pyparser\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/7c/77a059dcf29b39e6c4315ce9e9c7d4959be6f13af8ee42e6d5376b599015/pyparser-1.0.tar.gz\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (50.3.2)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from QKeras==0.8.0) (2.5)\n",
            "Collecting keras-tuner>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 27.0MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/54/115f0c28a61d56674c3a5e05c46d6c3523ad196e1dcd3e2d8b119026df36/tqdm-4.54.1-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.6MB/s \n",
            "\u001b[?25hCollecting parse==1.6.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/eb/44c70a5704fdf55d26a33070a9a13a03f0d66a5f6b72cadf75620d9dc4c0/parse-1.6.5.tar.gz\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (0.1.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->QKeras==0.8.0) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->QKeras==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (20.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->QKeras==0.8.0) (2.23.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.23.1->QKeras==0.8.0) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner>=1.0.1->QKeras==0.8.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->QKeras==0.8.0) (2.10)\n",
            "Building wheels for collected packages: QKeras, pyparser, keras-tuner, parse, terminaltables\n",
            "  Building wheel for QKeras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for QKeras: filename=QKeras-0.8.0-cp36-none-any.whl size=143338 sha256=a9011de1af2ebefc964c878699927cd01f752d746c75107efada6254d4334515\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v0rc_7n_/wheels/b4/74/1d/9456d62789716894a5edd7e342b4beaef69241ac584706c68d\n",
            "  Building wheel for pyparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyparser: filename=pyparser-1.0-cp36-none-any.whl size=4944 sha256=82f53d35cf753315286d05ef9689c0987b6e9b87cc9fbb0da86fe89811dee201\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/1c/4f/9f66cd69719aa41c2684efae758d95db7803e9fe1f65146db1\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=6ca6bdada773bff5297b7109efd8fdd3d87dcd79200ac068b755c43be0332faa\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.6.5-cp36-none-any.whl size=18178 sha256=005a79c827704c80c8b929d6b77d25d488078fe3e434e64644dc0bae853b0bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/cd/20/b1fc8e3046811c0d10f03d1027c26077c9ca396cf8d579f4e3\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=5d60c43f61eedbbe944c1b0d443727fc7a14de6fdce4738fee3f7970a9755411\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built QKeras pyparser keras-tuner parse terminaltables\n",
            "Installing collected packages: parse, pyparser, tensorflow-model-optimization, terminaltables, colorama, tqdm, threadpoolctl, scikit-learn, keras-tuner, QKeras\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed QKeras-0.8.0 colorama-0.4.4 keras-tuner-1.0.2 parse-1.6.5 pyparser-1.0 scikit-learn-0.23.2 tensorflow-model-optimization-0.5.0 terminaltables-3.1.0 threadpoolctl-2.1.0 tqdm-4.54.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP79yT967qPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8d1ad2-f556-418c-b7da-77904e70d503"
      },
      "source": [
        "def get_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
        "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
        "\n",
        "    x_train /= 256.0\n",
        "    x_test /= 256.0\n",
        "\n",
        "    x_mean = np.mean(x_train, axis=0)\n",
        "\n",
        "    x_train -= x_mean\n",
        "    x_test -= x_mean\n",
        "\n",
        "    nb_classes = np.max(y_train)+1\n",
        "    y_train = to_categorical(y_train, nb_classes)\n",
        "    y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T86NjHsL8qmy"
      },
      "source": [
        "from qkeras import *\n",
        "import qkeras\n",
        "def CreateQModel(shape, nb_classes, intBits):\n",
        "    x = x_in = Input(shape)\n",
        "\n",
        "    x = Flatten(name=\"flatten\")(x)\n",
        "    x = QDense(256,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha = 1)\".format(intBits),\n",
        "        name=\"dense\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_1\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense2\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_2\")(x)\n",
        "    x = QDense(128,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense3\")(x)\n",
        "    x = QActivation(\"quantized_tanh(9, {})\".format(intBits), name=\"act_3\")(x)\n",
        "    x = QDense(nb_classes,\n",
        "        kernel_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        bias_quantizer=\"quantized_bits(9, {} , alpha=1)\".format(intBits),\n",
        "        name=\"dense4\")(x)\n",
        "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeuD8FB58vlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afbcff2-b46b-4ccf-d36c-ab6f18d1a6d4"
      },
      "source": [
        "from qkeras.utils import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=2, restore_best_weights=True)\n",
        "qmodel = CreateQModel(x_train.shape[1:], y_train.shape[-1], 1)\n",
        "qmodel.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=Adam(0.0005),\n",
        "    metrics=[\"accuracy\"])\n",
        "history = qmodel.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test[:5000], y_test[:5000]), verbose=False, callbacks=[callback])\n",
        "model_save_quantized_weights(qmodel)\n",
        "print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... quantizing model\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxYVsQWR8z3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7412cf2-e656-4aae-f586-7ee8d49c2a93"
      },
      "source": [
        "qmodel.evaluate(x_test[5000:], y_test[5000:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06279278546571732, 0.9828000068664551]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUkqdNE84Xw"
      },
      "source": [
        "tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n",
        "lookup_arctanh = np.arctanh(tangents)\n",
        "\n",
        "def cordicTanh(x, iterations):\n",
        "  '''\n",
        "  Returns the quantized tanh of the supplied argument x\n",
        "  '''\n",
        "  \n",
        "\n",
        "  global  lookup_arctanh\n",
        "  current_vector = np.array([1.2075, 0])\n",
        "  z = x\n",
        "\n",
        "  for i in range(1, iterations+1):\n",
        "    m = -1\n",
        "    sigma = np.sign(z)\n",
        "    x = current_vector[0]\n",
        "    y = current_vector[1]\n",
        "    xnew = x\n",
        "    ynew = y\n",
        "    xnew = xnew - m*sigma*2**(-i)*y\n",
        "    ynew = ynew + sigma*2**(-i)*x\n",
        "    z = z - sigma*lookup_arctanh[i-1]\n",
        "    current_vector = [xnew, ynew]\n",
        "\n",
        "  \n",
        "  ex = current_vector[0] + current_vector[1]\n",
        "  eminusx = current_vector[0] - current_vector[1]\n",
        "\n",
        "  temp1 = quantized_bits(9,1,alpha=1)(ex - eminusx)\n",
        "  temp2 = quantized_bits(9, 1, alpha=1)(ex + eminusx)\n",
        "\n",
        "  return quantized_bits(9,1, alpha=1)(temp1/temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b98gLU6K9_Df"
      },
      "source": [
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "justAHolder = 0\n",
        "numOfIterations = 0\n",
        "\n",
        "def calcAndStore(argument):\n",
        "  global justAHolder\n",
        "  i = argument[0]\n",
        "  j = argument[1]\n",
        "  justAHolder[i][j] = cordicTanh(justAHolder[i][j], numOfIterations)\n",
        "\n",
        "def doit(arguments):\n",
        "  if __name__ == '__main__':\n",
        "    pool = ThreadPool(13)\n",
        "    pool.map(calcAndStore, arguments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5c5BVvuGvl2"
      },
      "source": [
        "tangents = 2**(-1*np.arange(1.0, 100.0, 1.0))\n",
        "lookup_arctanh = np.arctanh(tangents)\n",
        "\n",
        "def modifiedCordicTanh(arr, iterations):\n",
        "  '''\n",
        "  Returns the quantized tanh of the supplied argument x\n",
        "  '''\n",
        "  xarr = 1.2075*np.ones(shape=(len(arr), len(arr[0])))\n",
        "  yarr = np.zeros(shape = (len(arr), len(arr[0])))\n",
        "\n",
        "  global  lookup_arctanh\n",
        "  #current_vector = np.array([1.2075, 0])\n",
        "  #z = x\n",
        "\n",
        "  for i in range(1, iterations+1):\n",
        "    m = -1\n",
        "    sigma = np.sign(arr)\n",
        "    \n",
        "    xchange = m*sigma*2**(-i)*yarr\n",
        "    ychange = sigma*2**(-i)*xarr\n",
        "    arrchange = sigma*lookup_arctanh[i-1]\n",
        "\n",
        "    xarr -= xchange\n",
        "    yarr += ychange\n",
        "    arr -= arrchange\n",
        "    \n",
        "    #x = current_vector[0]\n",
        "    #y = current_vector[1]\n",
        "    #xnew = x\n",
        "    #ynew = y\n",
        "    #xnew = xnew - m*sigma*2**(-i)*y\n",
        "    #ynew = ynew + sigma*2**(-i)*x\n",
        "    #z = z - sigma*lookup_arctanh[i-1]\n",
        "    #current_vector = [xnew, ynew]\n",
        "\n",
        "  ex = xarr + yarr\n",
        "  eminusx = xarr - yarr\n",
        "\n",
        "  temp1 = quantized_bits(9, 1, alpha=1)(ex - eminusx)\n",
        "  temp2 = quantized_bits(9, 1, alpha=1)(ex + eminusx)\n",
        "\n",
        "  return quantized_bits(9,1, alpha=1)(temp1/temp2)\n",
        "\n",
        "  #ex = current_vector[0] + current_vector[1]\n",
        "  #eminusx = current_vector[0] - current_vector[1]\n",
        "\n",
        "  #temp1 = quantized_bits(9,1,alpha=1)(ex - eminusx)\n",
        "  #temp2 = quantized_bits(9, 1, alpha=1)(ex + eminusx)\n",
        "\n",
        "  #return quantized_bits(9,1, alpha=1)(temp1/temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnsSBXSD-F9m"
      },
      "source": [
        "x_test = x_test[5000:]\n",
        "y_test = y_test[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMXQKF7Q-Jbr"
      },
      "source": [
        "def epicGeneratePredictions(indices, iterations):\n",
        "  global justAHolder, numOfIterations\n",
        "  from keras import backend as K\n",
        "  get_third_layer_output = K.function([qmodel.layers[0].input],\n",
        "                                    [qmodel.layers[2].output])\n",
        " \n",
        "  layer3_output = get_third_layer_output([x_test])[0]\n",
        "\n",
        "  layer3_output = layer3_output[indices[0]: indices[1]]\n",
        "\n",
        "  layer3_output = modifiedCordicTanh(layer3_output, iterations)\n",
        "\n",
        "  ### Let us try and sidestep this ###\n",
        "  #for i in range(len(layer3_output)):\n",
        "  #  for j in range(len(layer3_output[0])):\n",
        "  #    layer3_output[i][j] = cordicSigmoid(layer3_output[i][j], iterations)\n",
        "\n",
        "  #justAHolder = layer3_output\n",
        "  #numOfIterations = iterations\n",
        "  #arguments = np.dstack(np.meshgrid(np.array(range(len(layer3_output))), range(len(layer3_output[0])))).reshape(-1, 2)\n",
        "  #doit(arguments)\n",
        "\n",
        "  #layer3_output = justAHolder\n",
        "\n",
        "  #print (\"Done!!!\")\n",
        "\n",
        "  input_shape = qmodel.layers[4].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(256))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[4](x)\n",
        "  qm4 = Model(layer_input, x)\n",
        "\n",
        "  predictions = qm4.predict(layer3_output)\n",
        "\n",
        "  predictions = modifiedCordicTanh(predictions, iterations)\n",
        "\n",
        "  #justAHolder = predictions\n",
        "  #numOfIterations = iterations\n",
        "  #arguments = np.dstack(np.meshgrid(np.array(range(len(predictions))), range(len(predictions[0])))).reshape(-1, 2)\n",
        "  #doit(arguments)\n",
        "\n",
        "  #predictions = justAHolder\n",
        "\n",
        "  #for i in range(len(predictions)):\n",
        "  #  for j in range(len(predictions[0])):\n",
        "  #    predictions[i][j] = cordicSigmoid(predictions[i][j], iterations)\n",
        "\n",
        "  input_shape = qmodel.layers[6].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(128))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[6](x)\n",
        "  qm4 = Model(layer_input, x)\n",
        "\n",
        "  predictions = qm4.predict(predictions)\n",
        "\n",
        "\n",
        "  predictions = modifiedCordicTanh(predictions, iterations)\n",
        "  #justAHolder = predictions\n",
        "  #numOfIterations = iterations\n",
        "  #arguments = np.dstack(np.meshgrid(np.array(range(len(predictions))), range(len(predictions[0])))).reshape(-1, 2)\n",
        "  #doit(arguments)\n",
        "\n",
        "  #predictions = justAHolder\n",
        "\n",
        "  #for i in range(len(predictions)):\n",
        "  #  for j in range(len(predictions[0])):\n",
        "  #    predictions[i][j] = cordicSigmoid(predictions[i][j], iterations)\n",
        "\n",
        "  input_shape = qmodel.layers[8].get_input_shape_at(0)\n",
        "  layer_input = Input(shape=(128))\n",
        "  x = layer_input\n",
        "  x = qmodel.layers[8](x)\n",
        "\n",
        "  qm4 = Model(layer_input, x)\n",
        "  predictions = np.array(qm4.predict(predictions))\n",
        "\n",
        "  #answer = np.zeros_like(predictions)\n",
        "  #answer[len(predictions), predictions.argmax()] = 1\n",
        "  a = predictions\n",
        "  return (a == a.max(axis=1)[:,None]).astype(int)\n",
        "  #return answer\n",
        "\n",
        "  #return answer\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ezQHcXW-Mrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b590e10-bd5b-4821-a55f-5d340c2f90b8"
      },
      "source": [
        "answerVectors = []\n",
        "for iter in [2,3,4, 5, 6, 7]:\n",
        "  answerVectors.append(epicGeneratePredictions([0, 5000], iter))\n",
        "  print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmxRo0p3JTMi",
        "outputId": "15b5c7fb-b49e-4ecf-ba34-109fb8a0d5ae"
      },
      "source": [
        "accuracy = []\n",
        "for i in answerVectors:\n",
        "  correct = 0\n",
        "  for j in range(1000):\n",
        "    if (i[j] == y_test[j]).all():\n",
        "      correct += 1\n",
        "  accuracy.append(correct)\n",
        "\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[974, 982, 981, 981, 981, 981]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgMFbHW_uul",
        "outputId": "792b8a5d-a275-46ca-d037-a4982e414c8d"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MaBuiil_2DE",
        "outputId": "df4596fb-43f6-40b2-f805-1368f8519fbc"
      },
      "source": [
        "answerVectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}