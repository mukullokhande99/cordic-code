{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#mobile VIT v1\n"
      ],
      "metadata": {
        "id": "Fycfz1o41ai6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DeBGtiis7st",
        "outputId": "a8c74f78-3d0c-4116-b099-d4397662ef77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m41.0/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m959.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Collecting tabulate (from fvcore)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=5a110aaf32bd14beac594cb768623d84400e1ffd991249647d8e8f15e458a579\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=76071991f70bd62bd57f4eef14afe620a21eb593923dbb388a34b993f4957b5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, tabulate, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 tabulate-0.9.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdWvPoIjx8np"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "global softmax_counter_v1\n",
        "softmax_counter_v1=0\n",
        "global softmax_counter_attention_v1\n",
        "softmax_counter_attention_v1=[]\n",
        "global af_count\n",
        "af_count=0\n",
        "AF_count_v1=[]\n",
        "def conv_2d(inp, oup, kernel_size=3, stride=1, padding=0, groups=1, bias=False, norm=True, act=True):\n",
        "    conv = nn.Sequential()\n",
        "    conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
        "    if norm:\n",
        "        conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
        "    if act:\n",
        "        global af_count\n",
        "        #print(oup)\n",
        "        af_count+=oup\n",
        "        AF_count_v1.append(oup)\n",
        "        conv.add_module('Activation', nn.SiLU())\n",
        "    return conv\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "        # hidden_dim = int(round(inp * expand_ratio))\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.block = nn.Sequential()\n",
        "        if expand_ratio != 1:\n",
        "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0))\n",
        "        self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim))\n",
        "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, padding=0, act=False))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.block(x)\n",
        "        else:\n",
        "            return self.block(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=4, dim_head=8, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        b_sz, S_len, in_channels = x.shape\n",
        "        # self-attention\n",
        "        # [N, S, C] --> [N, S, 3C] --> [N, S, 3, h, c] where C = hc\n",
        "        qkv = self.qkv_proj(x).reshape(b_sz, S_len, 3, self.num_heads, -1)\n",
        "        # [N, S, 3, h, c] --> [N, h, 3, S, C]\n",
        "        qkv = qkv.transpose(1, 3).contiguous()\n",
        "        # [N, h, 3, S, C] --> [N, h, S, C] x 3\n",
        "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        # [N h, T, c] --> [N, h, c, T]\n",
        "        k = k.transpose(-1, -2)\n",
        "        # QK^T\n",
        "        # [N, h, S, c] x [N, h, c, T] --> [N, h, S, T]\n",
        "        attn = torch.matmul(q, k)\n",
        "        batch_size, num_heads, num_src_tokens, num_tgt_tokens = attn.shape\n",
        "        attn_dtype = attn.dtype\n",
        "        attn_as_float = self.softmax(attn.float())\n",
        "        ###################################################################\n",
        "        print(\"Softmax called in Attention class\")\n",
        "        global softmax_counter_attention_v1\n",
        "        softmax_counter_attention_v1.append(attn_as_float)\n",
        "        global softmax_counter_v1\n",
        "        softmax_counter_v1 += 1\n",
        "        ####################################################################\n",
        "        attn = attn_as_float.to(attn_dtype)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # weighted sum\n",
        "        # [N, h, S, T] x [N, h, T, c] --> [N, h, S, c]\n",
        "        out = torch.matmul(attn, v)\n",
        "        # [N, h, S, c] --> [N, S, h, c] --> [N, S, C]\n",
        "        out = out.transpose(1, 2).reshape(b_sz, S_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, ffn_latent_dim, heads=8, dim_head=8, dropout=0, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.pre_norm_mha = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim, eps=1e-5, elementwise_affine=True),\n",
        "            Attention(embed_dim, heads, dim_head, attn_dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.pre_norm_ffn = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim, eps=1e-5, elementwise_affine=True),\n",
        "            nn.Linear(embed_dim, ffn_latent_dim, bias=True),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_latent_dim, embed_dim, bias=True),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        AF_count_v1.append(ffn_latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention\n",
        "        x = x + self.pre_norm_mha(x)\n",
        "        # Feed Forward network\n",
        "        x = x + self.pre_norm_ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, inp, attn_dim, ffn_multiplier, heads, dim_head, attn_blocks, patch_size):\n",
        "        super(MobileViTBlock, self).__init__()\n",
        "        self.patch_h, self.patch_w = patch_size\n",
        "        self.patch_area = int(self.patch_h * self.patch_w)\n",
        "\n",
        "        # local representation\n",
        "        self.local_rep = nn.Sequential()\n",
        "        self.local_rep.add_module('conv_3x3', conv_2d(inp, inp, kernel_size=3, stride=1, padding=1))\n",
        "        self.local_rep.add_module('conv_1x1', conv_2d(inp, attn_dim, kernel_size=1, stride=1, norm=False, act=False))\n",
        "\n",
        "        # global representation\n",
        "        self.global_rep = nn.Sequential()\n",
        "        ffn_dims = [int((ffn_multiplier*attn_dim)//16*16)] * attn_blocks\n",
        "        for i in range(attn_blocks):\n",
        "            ffn_dim = ffn_dims[i]\n",
        "            self.global_rep.add_module(f'TransformerEncoder_{i}', TransformerEncoder(attn_dim, ffn_dim, heads, dim_head))\n",
        "        self.global_rep.add_module('LayerNorm', nn.LayerNorm(attn_dim, eps=1e-5, elementwise_affine=True))\n",
        "\n",
        "        self.conv_proj = conv_2d(attn_dim, inp, kernel_size=1, stride=1)\n",
        "        self.fusion = conv_2d(2*inp, inp, kernel_size=3, stride=1)\n",
        "\n",
        "    def unfolding(self, feature_map):\n",
        "        patch_w, patch_h = self.patch_w, self.patch_h\n",
        "        batch_size, in_channels, orig_h, orig_w = feature_map.shape\n",
        "\n",
        "        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)\n",
        "        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)\n",
        "\n",
        "        interpolate = False\n",
        "        if new_w != orig_w or new_h != orig_h:\n",
        "            # Note: Padding can be done, but then it needs to be handled in attention function.\n",
        "            feature_map = F.interpolate(\n",
        "                feature_map, size=(new_h, new_w), mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "            interpolate = True\n",
        "\n",
        "        # number of patches along width and height\n",
        "        num_patch_w = new_w // patch_w  # n_w\n",
        "        num_patch_h = new_h // patch_h  # n_h\n",
        "        num_patches = num_patch_h * num_patch_w  # N\n",
        "\n",
        "        # [B, C, H, W] --> [B * C * n_h, p_h, n_w, p_w]\n",
        "        reshaped_fm = feature_map.reshape(\n",
        "            batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w\n",
        "        )\n",
        "        # [B * C * n_h, p_h, n_w, p_w] --> [B * C * n_h, n_w, p_h, p_w]\n",
        "        transposed_fm = reshaped_fm.transpose(1, 2)\n",
        "        # [B * C * n_h, n_w, p_h, p_w] --> [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n",
        "        reshaped_fm = transposed_fm.reshape(\n",
        "            batch_size, in_channels, num_patches, self.patch_area\n",
        "        )\n",
        "        # [B, C, N, P] --> [B, P, N, C]\n",
        "        transposed_fm = reshaped_fm.transpose(1, 3)\n",
        "        # [B, P, N, C] --> [BP, N, C]\n",
        "        patches = transposed_fm.reshape(batch_size * self.patch_area, num_patches, -1)\n",
        "\n",
        "        info_dict = {\n",
        "            \"orig_size\": (orig_h, orig_w),\n",
        "            \"batch_size\": batch_size,\n",
        "            \"interpolate\": interpolate,\n",
        "            \"total_patches\": num_patches,\n",
        "            \"num_patches_w\": num_patch_w,\n",
        "            \"num_patches_h\": num_patch_h,\n",
        "        }\n",
        "\n",
        "        return patches, info_dict\n",
        "\n",
        "    def folding(self, patches, info_dict):\n",
        "        n_dim = patches.dim()\n",
        "        assert n_dim == 3, \"Tensor should be of shape BPxNxC. Got: {}\".format(\n",
        "            patches.shape\n",
        "        )\n",
        "        # [BP, N, C] --> [B, P, N, C]\n",
        "        patches = patches.contiguous().view(\n",
        "            info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1\n",
        "        )\n",
        "\n",
        "        batch_size, pixels, num_patches, channels = patches.size()\n",
        "        num_patch_h = info_dict[\"num_patches_h\"]\n",
        "        num_patch_w = info_dict[\"num_patches_w\"]\n",
        "\n",
        "        # [B, P, N, C] --> [B, C, N, P]\n",
        "        patches = patches.transpose(1, 3)\n",
        "\n",
        "        # [B, C, N, P] --> [B*C*n_h, n_w, p_h, p_w]\n",
        "        feature_map = patches.reshape(\n",
        "            batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w\n",
        "        )\n",
        "        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w]\n",
        "        feature_map = feature_map.transpose(1, 2)\n",
        "        # [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n",
        "        feature_map = feature_map.reshape(\n",
        "            batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w\n",
        "        )\n",
        "        if info_dict[\"interpolate\"]:\n",
        "            feature_map = F.interpolate(\n",
        "                feature_map,\n",
        "                size=info_dict[\"orig_size\"],\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "        return feature_map\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x.clone()\n",
        "        x = self.local_rep(x)\n",
        "        x, info_dict = self.unfolding(x)\n",
        "        x = self.global_rep(x)\n",
        "        x = self.folding(x, info_dict)\n",
        "        x = self.conv_proj(x)\n",
        "        x = self.fusion(torch.cat((res, x), dim=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, mode, num_classes, patch_size=(2, 2)):\n",
        "        super().__init__()\n",
        "        # check image size\n",
        "        ih, iw = image_size\n",
        "        self.ph, self.pw = patch_size\n",
        "        assert ih % self.ph == 0 and iw % self.pw == 0\n",
        "        assert mode in ['xx_small', 'x_small', 'small']\n",
        "\n",
        "        # model size\n",
        "        if mode == 'xx_small':\n",
        "            mv2_exp_mult = 2\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 4\n",
        "            channels = [16, 16, 24, 48, 64, 80]\n",
        "            attn_dim = [64, 80, 96]\n",
        "        elif mode == 'x_small':\n",
        "            mv2_exp_mult = 4\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 4\n",
        "            channels = [16, 32, 48, 64, 80, 96]\n",
        "            attn_dim = [96, 120, 144]\n",
        "        elif mode == 'small':\n",
        "            mv2_exp_mult = 4\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 4\n",
        "            channels = [16, 32, 64, 96, 128, 160]\n",
        "            attn_dim = [144, 192, 240]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.conv_0 = conv_2d(3, channels[0], kernel_size=3, stride=2)\n",
        "\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            InvertedResidual(channels[0], channels[1], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            InvertedResidual(channels[1], channels[2], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            InvertedResidual(channels[2], channels[3], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlock(channels[3], attn_dim[0], ffn_multiplier, heads=4, dim_head=8, attn_blocks=2, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            InvertedResidual(channels[3], channels[4], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlock(channels[4], attn_dim[1], ffn_multiplier, heads=4, dim_head=8, attn_blocks=4, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_5 = nn.Sequential(\n",
        "            InvertedResidual(channels[4], channels[5], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlock(channels[5], attn_dim[2], ffn_multiplier, heads=4, dim_head=8, attn_blocks=3, patch_size=patch_size)\n",
        "        )\n",
        "        self.conv_1x1_exp = conv_2d(channels[-1], channels[-1]*last_layer_exp_factor, kernel_size=1, stride=1)\n",
        "        self.out = nn.Linear(channels[-1]*last_layer_exp_factor, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_0(x)\n",
        "        x = self.layer_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        x = self.layer_5(x)\n",
        "        x = self.conv_1x1_exp(x)\n",
        "\n",
        "        # FF head\n",
        "        x = torch.mean(x, dim=[-2, -1])\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "model = MobileViT(\n",
        "    image_size = (224,224),\n",
        "    mode = 'small',             # support [\"xx_small\", \"x_small\", \"small\"] as shown in paper\n",
        "    num_classes=1000,\n",
        "    patch_size=(2, 2)\n",
        ")\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "flop_analyzer = FlopCountAnalysis(model, x)\n",
        "print(flop_count_table(flop_analyzer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjnf_WBqyI6t",
        "outputId": "714a0db1-277f-46bf-e7ee-2f12fa89df06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "| module                                  | #parameters or shape   | #flops     |\n",
            "|:----------------------------------------|:-----------------------|:-----------|\n",
            "| model                                   | 5.579M                 | 1.455G     |\n",
            "|  conv_0                                 |  0.464K                |  6.308M    |\n",
            "|   conv_0.conv                           |   0.432K               |   5.323M   |\n",
            "|    conv_0.conv.weight                   |    (16, 3, 3, 3)       |            |\n",
            "|   conv_0.BatchNorm2d                    |   32                   |   0.986M   |\n",
            "|    conv_0.BatchNorm2d.weight            |    (16,)               |            |\n",
            "|    conv_0.BatchNorm2d.bias              |    (16,)               |            |\n",
            "|  layer_1.0.block                        |  3.968K                |  54.804M   |\n",
            "|   layer_1.0.block.exp_1x1               |   1.152K               |   16.559M  |\n",
            "|    layer_1.0.block.exp_1x1.conv         |    1.024K              |    12.617M |\n",
            "|    layer_1.0.block.exp_1x1.BatchNorm2d  |    0.128K              |    3.943M  |\n",
            "|   layer_1.0.block.conv_3x3              |   0.704K               |   11.04M   |\n",
            "|    layer_1.0.block.conv_3x3.conv        |    0.576K              |    7.097M  |\n",
            "|    layer_1.0.block.conv_3x3.BatchNorm2d |    0.128K              |    3.943M  |\n",
            "|   layer_1.0.block.red_1x1               |   2.112K               |   27.205M  |\n",
            "|    layer_1.0.block.red_1x1.conv         |    2.048K              |    25.233M |\n",
            "|    layer_1.0.block.red_1x1.BatchNorm2d  |    64                  |    1.971M  |\n",
            "|  layer_2                                |  86.528K               |  0.329G    |\n",
            "|   layer_2.0.block                       |   14.08K               |   90.666M  |\n",
            "|    layer_2.0.block.exp_1x1              |    4.352K              |    58.352M |\n",
            "|    layer_2.0.block.conv_3x3             |    1.408K              |    5.62M   |\n",
            "|    layer_2.0.block.red_1x1              |    8.32K               |    26.694M |\n",
            "|   layer_2.1.block                       |   36.224K              |   0.119G   |\n",
            "|    layer_2.1.block.exp_1x1              |    16.896K             |    55.394M |\n",
            "|    layer_2.1.block.conv_3x3             |    2.816K              |    11.239M |\n",
            "|    layer_2.1.block.red_1x1              |    16.512K             |    52.384M |\n",
            "|   layer_2.2.block                       |   36.224K              |   0.119G   |\n",
            "|    layer_2.2.block.exp_1x1              |    16.896K             |    55.394M |\n",
            "|    layer_2.2.block.conv_3x3             |    2.816K              |    11.239M |\n",
            "|    layer_2.2.block.red_1x1              |    16.512K             |    52.384M |\n",
            "|  layer_3                                |  0.657M                |  0.629G    |\n",
            "|   layer_3.0.block                       |   44.48K               |   77.848M  |\n",
            "|    layer_3.0.block.exp_1x1              |    16.896K             |    55.394M |\n",
            "|    layer_3.0.block.conv_3x3             |    2.816K              |    2.81M   |\n",
            "|    layer_3.0.block.red_1x1              |    24.768K             |    19.644M |\n",
            "|   layer_3.1                             |   0.612M               |   0.551G   |\n",
            "|    layer_3.1.local_rep                  |    96.96K              |    76.242M |\n",
            "|    layer_3.1.global_rep                 |    0.335M              |    0.351G  |\n",
            "|    layer_3.1.conv_proj                  |    14.016K             |    11.214M |\n",
            "|    layer_3.1.fusion                     |    0.166M              |    0.112G  |\n",
            "|  layer_4                                |  1.772M                |  0.353G    |\n",
            "|   layer_4.0.block                       |   91.264K              |   35.541M  |\n",
            "|    layer_4.0.block.exp_1x1              |    37.632K             |    26.218M |\n",
            "|    layer_4.0.block.conv_3x3             |    4.224K              |    0.909M  |\n",
            "|    layer_4.0.block.red_1x1              |    49.408K             |    8.415M  |\n",
            "|   layer_4.1                             |   1.681M               |   0.317G   |\n",
            "|    layer_4.1.local_rep                  |    0.172M              |    29.182M |\n",
            "|    layer_4.1.global_rep                 |    1.188M              |    0.248G  |\n",
            "|    layer_4.1.conv_proj                  |    24.832K             |    4.262M  |\n",
            "|    layer_4.1.fusion                     |    0.295M              |    35.762M |\n",
            "|  layer_5                                |  2.314M                |  80.513M   |\n",
            "|   layer_5.0.block                       |   0.154M               |   11.476M  |\n",
            "|    layer_5.0.block.exp_1x1              |    66.56K              |    8.24M   |\n",
            "|    layer_5.0.block.conv_3x3             |    5.632K              |    0.258M  |\n",
            "|    layer_5.0.block.red_1x1              |    82.24K              |    2.978M  |\n",
            "|   layer_5.1                             |   2.16M                |   69.038M  |\n",
            "|    layer_5.1.local_rep                  |    0.269M              |    9.706M  |\n",
            "|    layer_5.1.global_rep                 |    1.391M              |    50.535M |\n",
            "|    layer_5.1.conv_proj                  |    38.72K              |    1.411M  |\n",
            "|    layer_5.1.fusion                     |    0.461M              |    7.386M  |\n",
            "|  conv_1x1_exp                           |  0.104M                |  1.69M     |\n",
            "|   conv_1x1_exp.conv                     |   0.102M               |   1.638M   |\n",
            "|    conv_1x1_exp.conv.weight             |    (640, 160, 1, 1)    |            |\n",
            "|   conv_1x1_exp.BatchNorm2d              |   1.28K                |   51.2K    |\n",
            "|    conv_1x1_exp.BatchNorm2d.weight      |    (640,)              |            |\n",
            "|    conv_1x1_exp.BatchNorm2d.bias        |    (640,)              |            |\n",
            "|  out                                    |  0.641M                |  0.64M     |\n",
            "|   out.weight                            |   (1000, 640)          |            |\n",
            "|   out.bias                              |   (1000,)              |            |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "softmax_counter_attention_v1=[]\n",
        "softmax_counter_v1=0\n",
        "af_count=0\n",
        "model = MobileViT(\n",
        "    image_size = (224,224),\n",
        "    mode = 'small',             # support [\"xx_small\", \"x_small\", \"small\"] as shown in paper\n",
        "    num_classes=1000,\n",
        "    patch_size=(2, 2)\n",
        ")\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x) # (5, 1000)\n",
        "print(af_count)"
      ],
      "metadata": {
        "id": "VxM3vtS1yPqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd111f2-a200-455b-d37a-ba9dd5ab0452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "64\n",
            "64\n",
            "128\n",
            "128\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "96\n",
            "96\n",
            "96\n",
            "384\n",
            "384\n",
            "128\n",
            "128\n",
            "128\n",
            "512\n",
            "512\n",
            "160\n",
            "160\n",
            "160\n",
            "640\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "Softmax called in Attention class\n",
            "5520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out)\n",
        "print(softmax_counter_v1,len(softmax_counter_attention_v1))"
      ],
      "metadata": {
        "id": "iabdMq4NyVyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8939ac06-bba1-48a4-a8a8-540e50a3a871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-5.0546e-02,  8.3181e-02,  6.3606e-02, -9.6271e-02, -1.5694e-01,\n",
            "         -2.1863e-01, -1.9553e-02, -2.1763e-02,  1.1867e-01, -2.0760e-02,\n",
            "          1.6818e-02,  2.6220e-01, -2.4989e-02, -2.8180e-02,  8.4724e-02,\n",
            "          3.2357e-02, -5.8881e-02, -4.6160e-02, -3.1356e-01, -2.3410e-02,\n",
            "         -1.4248e-01,  2.5986e-02, -2.3364e-02,  8.8004e-02,  3.6296e-01,\n",
            "         -1.0998e-01, -9.0958e-02, -1.2928e-01,  1.6996e-01, -1.4220e-01,\n",
            "         -7.3337e-02,  2.2568e-01,  1.9845e-01,  4.2877e-02, -1.2177e-01,\n",
            "         -2.7980e-02, -2.8326e-02,  4.5885e-02, -9.4712e-03, -1.5252e-01,\n",
            "          4.0469e-02,  8.3243e-02,  1.7225e-01,  1.8099e-01,  1.5493e-03,\n",
            "         -1.6554e-01,  1.9599e-01,  8.9014e-03,  1.0225e-01, -1.7411e-02,\n",
            "         -7.5186e-02,  3.1610e-02, -3.7355e-02, -1.5365e-01, -1.6901e-01,\n",
            "          1.0180e-02,  1.3187e-01, -1.3877e-01, -9.3164e-02, -7.9840e-03,\n",
            "          1.3801e-02, -6.4508e-03, -2.4865e-02, -6.2089e-02, -1.4102e-01,\n",
            "         -2.5513e-02,  1.1706e-01,  3.9156e-02,  7.6335e-02, -3.2821e-02,\n",
            "          6.9377e-02,  2.3087e-02,  6.1559e-02,  4.6409e-03, -7.3560e-02,\n",
            "         -8.6846e-02, -1.5945e-01, -1.0739e-01, -1.4965e-01, -3.0142e-02,\n",
            "         -1.0550e-01, -1.4049e-01, -1.5544e-01, -1.6701e-01,  7.7575e-02,\n",
            "         -1.2947e-01, -1.0652e-02,  2.8557e-01,  7.2703e-02,  1.8006e-01,\n",
            "         -1.6791e-01,  1.6071e-02,  3.4884e-02,  1.7447e-01,  1.0132e-01,\n",
            "          5.1183e-02,  7.6165e-02,  3.9812e-02, -1.6940e-01, -3.5095e-01,\n",
            "         -5.6095e-03,  4.5904e-02, -4.7462e-02,  1.1280e-01,  2.8277e-01,\n",
            "          2.2898e-01,  8.7106e-02,  2.7885e-02,  9.4953e-02,  4.6958e-02,\n",
            "          1.0683e-01,  2.3728e-01,  2.1423e-01, -5.9597e-02, -1.4184e-01,\n",
            "         -7.6927e-02, -2.0887e-01,  9.0934e-02,  1.9542e-01,  1.4233e-01,\n",
            "          8.6085e-02,  1.8414e-02, -3.7619e-02,  1.4395e-01, -1.2202e-01,\n",
            "          2.0145e-02, -1.5675e-01,  4.8892e-03, -1.0850e-01,  1.3493e-01,\n",
            "         -1.2370e-01, -1.9908e-02,  8.1527e-02,  1.0027e-01, -1.3669e-01,\n",
            "         -1.2226e-01,  2.2539e-01, -5.2937e-02, -1.6369e-01,  1.6954e-01,\n",
            "          2.4165e-01, -1.6216e-01,  1.1785e-01, -1.5926e-01,  6.8229e-02,\n",
            "         -9.0763e-02,  1.3664e-01,  6.8818e-02, -4.9130e-02, -8.5338e-02,\n",
            "         -7.4182e-02,  5.1712e-02,  9.3130e-02, -6.6667e-02,  8.3673e-03,\n",
            "          2.6003e-01,  5.1243e-03,  1.8399e-01,  1.7110e-02,  2.1500e-01,\n",
            "         -2.3648e-01, -2.3103e-02, -3.1218e-01, -6.0682e-02,  1.0528e-03,\n",
            "         -2.1898e-01,  8.8454e-02,  1.2657e-01, -1.2773e-01,  3.2229e-02,\n",
            "         -5.4095e-02,  4.4741e-02,  3.3283e-02,  2.2692e-02,  5.5359e-02,\n",
            "          2.9994e-01, -2.9446e-01,  2.0165e-01, -1.8237e-01, -9.8764e-02,\n",
            "         -2.1028e-01,  1.5713e-01, -7.5752e-02, -1.1663e-01, -8.4670e-02,\n",
            "          1.5007e-01,  8.1510e-02,  9.3026e-03,  3.5550e-02, -2.6550e-02,\n",
            "          2.0039e-02, -2.6102e-01,  2.0443e-01,  3.5639e-02, -9.6931e-02,\n",
            "         -9.4518e-02, -8.9644e-02, -2.5015e-02, -1.3471e-01,  4.3386e-03,\n",
            "         -1.0307e-01, -3.0906e-01,  3.5362e-01,  1.2994e-02, -5.0593e-04,\n",
            "          1.4592e-01,  1.1839e-01, -3.3331e-02, -6.3294e-02,  1.5158e-01,\n",
            "          7.5746e-02,  5.4927e-02, -8.3326e-02, -2.2698e-01, -1.5520e-02,\n",
            "         -8.9449e-02,  3.5612e-02,  3.2563e-02, -8.0049e-02, -6.2237e-02,\n",
            "         -9.2237e-02,  4.2403e-02,  2.6391e-02,  1.6885e-01, -8.0289e-02,\n",
            "         -6.8207e-02,  5.9099e-02,  3.4042e-03,  7.3432e-02,  6.1595e-02,\n",
            "         -4.1591e-02, -1.1518e-01, -1.1643e-01, -1.9824e-02,  1.5200e-01,\n",
            "          1.7821e-01, -1.4275e-02, -6.8775e-02, -7.4484e-02, -2.3684e-01,\n",
            "         -1.6723e-01,  2.5963e-02, -2.2488e-01, -1.3475e-01,  1.8326e-01,\n",
            "          8.0647e-02,  2.3034e-02, -1.2695e-01,  1.0511e-01,  1.2402e-01,\n",
            "          8.8421e-02,  5.2484e-02, -8.0717e-02,  2.1134e-02,  5.2161e-02,\n",
            "          1.0452e-02, -7.2666e-02,  1.6074e-01,  2.4562e-02, -5.7664e-02,\n",
            "         -4.3153e-02, -9.5711e-02, -3.7039e-02, -1.6052e-01,  4.7389e-02,\n",
            "         -1.2409e-02, -1.3096e-02, -2.2565e-01, -2.0729e-01,  8.5034e-02,\n",
            "         -6.4551e-02, -1.2737e-02,  4.2717e-02, -1.7656e-01,  1.4678e-02,\n",
            "         -3.6661e-02, -8.3200e-02,  8.4743e-02,  7.1335e-02,  1.2515e-01,\n",
            "          1.2503e-01, -2.8181e-02,  2.7989e-02,  1.3975e-01,  2.4058e-01,\n",
            "         -8.8690e-02,  4.3621e-03,  7.5265e-02, -8.8346e-02, -1.4257e-01,\n",
            "         -3.5065e-02, -1.1941e-01, -1.2028e-01, -2.7563e-02, -2.6394e-01,\n",
            "         -9.7457e-02,  3.2896e-02, -1.9611e-01, -2.0126e-01,  1.4834e-01,\n",
            "         -2.7038e-02, -2.3484e-02, -3.8586e-02, -3.8415e-02, -7.9298e-02,\n",
            "         -1.5423e-01,  1.0438e-01,  6.6910e-02,  1.8049e-03, -1.1942e-01,\n",
            "          1.7644e-01,  5.9571e-02, -7.3850e-02,  2.9693e-02,  6.0755e-02,\n",
            "         -1.9086e-01,  1.3054e-01, -9.2896e-02, -2.3513e-01, -9.7029e-02,\n",
            "          1.1298e-01, -4.3433e-02,  1.7528e-01, -4.1066e-02,  1.0497e-01,\n",
            "          7.6985e-02, -1.5062e-01, -1.2530e-01, -9.7984e-02, -8.4681e-02,\n",
            "          7.7950e-04, -2.1945e-01,  1.7777e-02,  1.4939e-01,  2.6426e-02,\n",
            "         -2.5256e-01,  1.9732e-02,  1.9952e-02,  2.0541e-01,  1.4783e-01,\n",
            "         -2.6568e-02,  1.2946e-01, -1.3954e-01,  9.5628e-02,  2.0698e-02,\n",
            "         -9.3370e-02,  2.2136e-02, -1.7590e-01, -1.1405e-01,  1.2497e-01,\n",
            "         -8.9605e-02, -1.4951e-01, -1.7461e-01,  5.2765e-02,  3.6797e-02,\n",
            "         -1.4041e-01, -2.5721e-01,  6.1073e-02, -2.8763e-01, -7.4338e-02,\n",
            "          1.8004e-02,  1.0947e-01, -1.2648e-01, -1.8400e-01,  2.1554e-01,\n",
            "         -6.0934e-02, -1.7360e-01, -3.5874e-02,  2.0775e-01,  1.2432e-01,\n",
            "          1.1581e-01,  8.2979e-02, -1.8346e-01,  1.2871e-01, -7.3376e-02,\n",
            "         -1.3326e-02, -1.2184e-01,  1.8074e-01,  8.9121e-02,  1.4048e-01,\n",
            "         -1.2706e-02,  3.9651e-03, -2.6056e-03, -4.0271e-02,  1.4577e-01,\n",
            "         -1.7180e-01, -1.8329e-02,  4.0807e-02,  5.0807e-02,  9.1907e-02,\n",
            "          1.2944e-01, -8.2980e-02,  2.0154e-01, -5.6584e-02,  6.1071e-03,\n",
            "         -2.1965e-01, -1.6791e-02,  2.2042e-01,  7.9670e-03,  2.6612e-02,\n",
            "         -1.7068e-01, -1.8913e-01, -2.1534e-02,  1.4972e-01, -8.9107e-02,\n",
            "          1.5468e-01,  2.1936e-01,  2.6833e-01,  1.6692e-02,  1.1773e-01,\n",
            "          1.1995e-01, -1.6786e-01, -7.3638e-02, -1.3565e-02,  1.9636e-02,\n",
            "          1.5198e-01, -8.7538e-02, -1.4249e-01,  6.5049e-04,  3.6452e-02,\n",
            "         -3.3585e-02, -1.7939e-01, -3.4924e-02, -6.4900e-02,  1.1162e-01,\n",
            "          8.9400e-02,  1.0493e-01, -1.2303e-02,  1.3789e-02,  1.7663e-01,\n",
            "         -1.4621e-03,  1.3362e-03,  1.5906e-02,  2.8340e-01,  1.8289e-02,\n",
            "         -9.4155e-02,  8.6671e-02, -4.7837e-02,  1.0398e-02,  1.3259e-01,\n",
            "          4.4815e-02, -1.9579e-02,  9.5695e-02, -3.5442e-02,  2.5400e-02,\n",
            "          4.9077e-02, -1.9087e-01, -9.7958e-02, -6.7895e-02, -5.8131e-02,\n",
            "          3.3889e-02,  6.8084e-02, -6.1343e-02,  7.0328e-02,  9.5270e-02,\n",
            "          1.0202e-01, -1.1937e-01, -3.8072e-02, -7.2291e-03,  5.8968e-02,\n",
            "          7.3643e-02,  7.8632e-02, -2.9356e-03,  1.8453e-01,  2.2520e-01,\n",
            "         -1.3171e-01,  2.2702e-02,  2.0003e-02, -1.4188e-01, -8.7339e-02,\n",
            "         -7.9658e-02,  1.2006e-03, -2.6813e-01, -2.1307e-01, -1.3367e-01,\n",
            "         -3.2174e-02, -6.6934e-02, -1.7964e-01,  2.0540e-01,  1.2063e-01,\n",
            "          3.4578e-02, -1.0185e-01,  1.3083e-01, -2.2055e-02, -5.0364e-02,\n",
            "          6.8444e-02,  7.7138e-02,  2.0350e-01, -3.8830e-02,  2.6407e-01,\n",
            "          2.1592e-02, -9.3720e-02,  3.1290e-02, -8.0022e-02, -2.8718e-01,\n",
            "          4.6466e-02,  8.4454e-02,  1.0623e-01,  2.7100e-01, -2.8954e-01,\n",
            "          1.3999e-02, -1.3864e-02, -1.8948e-01,  9.8386e-02,  9.7948e-02,\n",
            "          1.7879e-02,  7.5523e-02, -7.9079e-02, -2.6397e-01,  1.2044e-01,\n",
            "          2.9864e-02,  1.0921e-01,  1.8877e-02, -1.4591e-01,  2.2765e-01,\n",
            "         -1.6196e-01,  1.8915e-01, -2.1481e-02,  3.0643e-02,  2.8143e-02,\n",
            "          1.6715e-02, -1.9751e-04, -2.5532e-02, -5.4975e-02,  5.6937e-02,\n",
            "          1.2676e-01, -4.2253e-02, -1.5979e-01, -2.3836e-02,  2.2741e-02,\n",
            "          2.0052e-01,  1.2623e-01, -1.2267e-01,  1.7132e-01, -1.4441e-01,\n",
            "         -4.3721e-02, -1.0026e-01, -4.3454e-02,  1.3663e-03, -7.7329e-02,\n",
            "          1.4632e-01,  5.9554e-02,  6.6688e-02,  2.7641e-01, -5.3746e-02,\n",
            "         -1.5958e-01, -4.9805e-02, -4.4325e-03,  8.3866e-02, -1.1521e-02,\n",
            "         -9.8855e-02,  1.8132e-02, -4.3030e-02, -1.8816e-02, -9.2651e-02,\n",
            "         -5.3144e-02, -1.4927e-01,  2.2778e-01, -3.1897e-02,  1.2377e-01,\n",
            "          3.3106e-01, -3.0749e-02, -6.8832e-03, -1.4147e-01,  1.7496e-01,\n",
            "          5.2023e-02, -1.7941e-02, -1.5322e-01, -6.5356e-02, -7.5031e-02,\n",
            "         -5.3836e-02, -6.3207e-02,  1.2338e-01,  1.2423e-01,  1.4682e-01,\n",
            "          1.8741e-01, -7.8100e-02, -2.9098e-01,  7.6212e-02, -1.3828e-01,\n",
            "          5.4450e-02,  1.1597e-01,  7.9733e-02,  5.8883e-02, -2.7833e-01,\n",
            "          1.2461e-01, -7.3178e-02, -1.9941e-02, -2.2736e-02, -4.6490e-02,\n",
            "         -5.1996e-02, -3.1045e-02,  3.9800e-02,  1.1542e-01,  1.1922e-02,\n",
            "          4.0771e-03,  3.8187e-02,  1.3474e-02, -3.0692e-01, -1.2800e-01,\n",
            "          1.1998e-02, -7.8540e-02,  8.3916e-02,  5.5131e-02,  1.3378e-01,\n",
            "         -1.4057e-01,  1.8131e-01,  3.9001e-02,  2.0455e-01,  5.3023e-03,\n",
            "          1.5238e-01, -5.9638e-02, -6.9226e-02,  4.5608e-02,  9.0066e-02,\n",
            "          2.3222e-02,  1.7980e-01,  8.8830e-02, -2.9889e-03, -1.3897e-01,\n",
            "         -1.2531e-01,  1.9100e-01,  1.8095e-01, -6.3482e-03,  2.1020e-01,\n",
            "          2.9928e-03, -5.7037e-03,  7.7132e-02,  1.5711e-01, -1.1552e-01,\n",
            "         -1.9469e-01, -2.9900e-01, -2.4876e-02,  2.2211e-02,  1.4307e-01,\n",
            "         -1.7398e-01,  7.5037e-02,  1.0102e-01,  1.4835e-01, -3.8766e-02,\n",
            "          1.4483e-02, -1.1034e-01,  4.0527e-02,  1.0331e-01,  4.7318e-02,\n",
            "          1.1996e-01, -1.6937e-01,  1.8462e-01, -1.8648e-01, -1.4183e-01,\n",
            "         -1.1947e-01, -1.3774e-01, -4.8004e-03,  1.3293e-02, -1.4776e-01,\n",
            "         -3.5810e-03, -1.8082e-02,  8.2750e-02, -1.3342e-01, -8.3059e-02,\n",
            "          2.2719e-01,  2.4924e-01,  1.0941e-01, -3.6152e-02, -1.3766e-01,\n",
            "          9.7895e-02,  1.7270e-01, -2.0765e-02,  4.6493e-02,  9.6509e-02,\n",
            "         -6.2602e-02, -3.3177e-02, -9.8756e-02, -1.6654e-01, -9.3316e-02,\n",
            "          2.0980e-01,  1.2355e-02, -2.3080e-02, -6.6765e-02,  6.0665e-02,\n",
            "          3.9735e-02, -2.0027e-01, -2.9429e-02, -7.6018e-02, -1.1350e-01,\n",
            "          9.6581e-02,  7.1523e-02, -1.1123e-01, -2.0922e-02, -9.1460e-03,\n",
            "          6.8399e-02, -5.3713e-02,  1.7231e-02,  1.9856e-01, -8.6313e-02,\n",
            "          1.1789e-01, -5.0094e-03, -8.7625e-02,  6.4009e-02, -3.8106e-03,\n",
            "          9.1096e-02,  9.5603e-03,  1.8886e-01, -2.4904e-01, -1.4703e-01,\n",
            "         -1.5667e-01,  2.9562e-01, -1.4663e-01, -8.0176e-02,  1.4573e-01,\n",
            "         -1.0507e-01,  3.4064e-02,  3.2413e-03,  2.2654e-01,  1.9489e-03,\n",
            "         -9.5579e-02, -9.9570e-02,  5.9560e-02,  1.6186e-01,  5.7021e-02,\n",
            "          2.4692e-02,  2.2532e-01,  1.1503e-01,  3.2451e-03,  5.5179e-02,\n",
            "         -6.2397e-02,  1.0086e-01, -6.5491e-02,  1.0355e-01, -1.1157e-02,\n",
            "          2.2668e-02,  1.0200e-01, -2.1275e-01,  6.6431e-02, -5.2289e-02,\n",
            "         -2.4792e-01,  2.8441e-01,  4.1594e-02, -7.1058e-02,  1.3635e-01,\n",
            "          1.1230e-01,  6.2736e-02,  7.3102e-03,  1.5680e-01,  4.7722e-02,\n",
            "         -1.1936e-01, -2.6975e-01, -1.1415e-01, -2.4194e-01, -3.4593e-02,\n",
            "         -1.1743e-01,  2.2310e-01,  3.0931e-02, -2.1407e-01,  1.1904e-01,\n",
            "          2.1072e-01,  1.1585e-01,  3.9806e-02, -8.5905e-02, -3.0818e-02,\n",
            "          2.6871e-02, -3.7443e-02, -1.6795e-01,  7.5213e-03, -1.6374e-01,\n",
            "          8.0319e-02, -3.1069e-02,  7.1059e-02, -6.0147e-02,  4.4437e-02,\n",
            "          8.0521e-02, -8.9920e-02,  8.7413e-02, -1.1650e-01, -1.1052e-01,\n",
            "          5.1797e-02, -5.0068e-02, -6.4608e-02, -3.9222e-02,  3.3802e-01,\n",
            "         -1.5959e-01,  1.4132e-01,  2.7686e-01, -3.7300e-02,  5.8370e-02,\n",
            "          1.3479e-01,  5.6502e-02,  7.3003e-02,  1.9210e-01,  2.1163e-02,\n",
            "          1.7077e-02, -4.9712e-02, -1.1748e-01, -5.0230e-02,  7.5113e-02,\n",
            "          3.3617e-02,  1.3114e-01,  6.6254e-02, -1.0091e-02, -3.3561e-02,\n",
            "         -1.0116e-01, -7.7128e-02,  8.3980e-02,  1.1239e-01,  9.7469e-02,\n",
            "          5.8595e-02, -8.9096e-02, -1.0118e-01,  9.1576e-02,  6.3689e-02,\n",
            "         -4.3052e-02,  1.0942e-02, -1.4288e-01, -1.6341e-02, -1.9852e-01,\n",
            "          2.1387e-01, -6.8505e-02, -9.0931e-02, -8.2160e-03, -1.2403e-01,\n",
            "          6.1940e-02, -3.6020e-02,  9.6432e-02,  1.5547e-01, -1.1723e-01,\n",
            "         -3.6484e-02,  1.2602e-01, -3.1682e-02, -8.4494e-02, -2.7971e-01,\n",
            "         -1.1204e-01, -3.7197e-02, -7.7267e-02, -1.2229e-01, -9.3053e-02,\n",
            "          1.2617e-02, -1.8355e-01, -2.7584e-01, -9.4934e-02, -9.3888e-02,\n",
            "          1.1687e-01, -7.4217e-02,  3.0347e-02, -6.4505e-02, -1.0099e-01,\n",
            "         -5.5352e-02,  5.6611e-02, -2.5323e-01, -1.1444e-01, -2.4818e-02,\n",
            "         -2.0378e-01, -7.6229e-02, -2.7906e-02,  1.3200e-01, -1.4758e-01,\n",
            "         -3.1130e-01,  2.1484e-01, -7.1857e-02,  1.9240e-01, -7.1914e-02,\n",
            "          8.4863e-02, -8.6486e-02, -1.0115e-01, -5.5553e-02,  2.2472e-01,\n",
            "         -7.2672e-02,  1.9322e-01, -3.4231e-02,  3.3231e-02,  2.5707e-02,\n",
            "         -1.4696e-01, -5.6106e-02, -1.5128e-02,  1.2462e-01, -1.2139e-01,\n",
            "         -3.8821e-03,  2.4330e-01, -4.6059e-02, -6.8068e-02,  7.6985e-02,\n",
            "          1.4798e-01,  8.8037e-02, -9.5230e-02, -2.0162e-02,  3.4894e-02,\n",
            "          2.0456e-01, -3.1086e-02, -1.7971e-01, -1.2294e-01, -1.0113e-01,\n",
            "         -7.9788e-02, -1.9883e-01, -9.4488e-02,  8.4312e-02, -6.8715e-02,\n",
            "         -1.8995e-02, -2.8275e-02, -3.7641e-02, -2.0363e-01,  4.4644e-02,\n",
            "         -1.4079e-04, -1.7096e-01, -7.6570e-02,  4.9716e-02,  5.0470e-02,\n",
            "          1.1744e-01,  7.4431e-02,  1.6814e-01,  1.0850e-01,  6.5356e-02,\n",
            "         -1.0170e-01, -1.2299e-01,  1.6092e-01,  7.5759e-03,  1.8225e-01,\n",
            "          3.8241e-02,  1.2547e-01,  1.0604e-01, -8.4986e-02,  9.6565e-02,\n",
            "          8.2898e-02, -8.6522e-02,  1.1705e-01, -7.2679e-02, -2.7324e-02,\n",
            "         -1.4976e-01, -6.3064e-02,  1.0530e-01,  3.9341e-02, -1.4750e-01,\n",
            "          1.2776e-01,  1.5040e-01, -2.4787e-02,  8.5469e-02, -4.7042e-02,\n",
            "          1.8564e-02,  2.3424e-01, -8.6774e-02,  2.3146e-01, -1.0477e-02,\n",
            "         -1.6123e-02, -2.0247e-01,  5.0192e-02, -9.7148e-02,  1.4444e-01,\n",
            "         -1.5382e-01,  6.9139e-02, -1.9489e-02,  1.2044e-01, -1.1560e-01,\n",
            "         -2.5690e-01, -1.2881e-01, -1.8470e-01, -1.3155e-01, -1.7104e-01,\n",
            "          6.3746e-02, -8.1780e-03,  1.0103e-01,  2.1319e-01,  5.6055e-02,\n",
            "          1.0467e-01, -2.1952e-01,  5.3783e-02,  1.1008e-01,  1.6456e-01,\n",
            "          1.4758e-01,  3.0878e-02,  2.1318e-01, -1.0059e-01, -3.1720e-02,\n",
            "          1.3775e-03,  7.1097e-02, -1.8124e-01,  1.0777e-02, -4.0756e-02,\n",
            "          5.2989e-03,  5.2489e-02, -3.3388e-01,  1.4600e-01,  7.2314e-03,\n",
            "         -3.7608e-02,  7.4909e-02,  1.1181e-01, -1.3784e-01, -1.4791e-01,\n",
            "          3.5215e-02,  1.4258e-01,  4.8117e-02,  1.7323e-01, -1.7767e-01,\n",
            "          4.4698e-02,  1.0788e-01,  1.3472e-02,  1.5294e-01, -1.5914e-01,\n",
            "         -1.5675e-02,  1.0371e-01, -2.2546e-01,  1.0166e-01,  9.7668e-03]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "9 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_count=0\n",
        "for i in (softmax_counter_attention_v1):\n",
        "    print(i.shape)\n",
        "    softmax_count+=i.shape[0]*i.shape[1]*i.shape[2]\n",
        "print(\"\\ntotal number of times the softmax is used is :- \",softmax_count)"
      ],
      "metadata": {
        "id": "sDOH7cT4Bx1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa1bf12-40ad-4b4e-df0a-cc0d3f52e0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4, 196, 196])\n",
            "torch.Size([4, 4, 196, 196])\n",
            "torch.Size([4, 4, 49, 49])\n",
            "torch.Size([4, 4, 49, 49])\n",
            "torch.Size([4, 4, 49, 49])\n",
            "torch.Size([4, 4, 49, 49])\n",
            "torch.Size([4, 4, 9, 9])\n",
            "torch.Size([4, 4, 9, 9])\n",
            "torch.Size([4, 4, 9, 9])\n",
            "\n",
            "total number of times the softmax is used is :-  9840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SiLU AF count\",sum(AF_count_v1)) ## silu count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAE-3qXtaw_A",
        "outputId": "ca424250-535a-4ffa-b96c-a7fe9d880186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SiLU AF count 18144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mobilevit v2"
      ],
      "metadata": {
        "id": "JxD7Cm68C6Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "softmax_counter_attention_v2=[]\n",
        "softmax_counter_v2=0\n",
        "\n",
        "def conv_2d(inp, oup, kernel_size=3, stride=1, padding=0, groups=1, bias=False, norm=True, act=True):\n",
        "    conv = nn.Sequential()\n",
        "    conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
        "    if norm:\n",
        "        conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
        "    if act:\n",
        "        conv.add_module('Activation', nn.SiLU())\n",
        "    return conv\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "        # hidden_dim = int(round(inp * expand_ratio))\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.block = nn.Sequential()\n",
        "        if expand_ratio != 1:\n",
        "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0))\n",
        "        self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim))\n",
        "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, padding=0, act=False))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.block(x)\n",
        "        else:\n",
        "            return self.block(x)\n",
        "\n",
        "\n",
        "class LinearSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = conv_2d(embed_dim, 1+2*embed_dim, kernel_size=1, bias=True, norm=False, act=False)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.out_proj = conv_2d(embed_dim, embed_dim, kernel_size=1, bias=True, norm=False, act=False)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = torch.split(qkv, split_size_or_sections=[1, self.embed_dim, self.embed_dim], dim=1)\n",
        "        context_score = F.softmax(q, dim=-1)\n",
        "        global softmax_counter_attention_v2\n",
        "        softmax_counter_attention_v2.append(context_score)\n",
        "        global softmax_counter_v2\n",
        "        softmax_counter_v2 += 1\n",
        "        context_score = self.attn_dropout(context_score)\n",
        "\n",
        "        context_vector = k * context_score\n",
        "        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)\n",
        "\n",
        "        out = F.relu(v) * context_vector.expand_as(v)\n",
        "        out = self.out_proj(out)\n",
        "        return out\n",
        "\n",
        "class LinearAttnFFN(nn.Module):\n",
        "    def __init__(self, embed_dim, ffn_latent_dim, dropout=0, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.pre_norm_attn = nn.Sequential(\n",
        "            nn.GroupNorm(num_channels=embed_dim, eps=1e-5, affine=True, num_groups=1),\n",
        "            LinearSelfAttention(embed_dim, attn_dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.pre_norm_ffn = nn.Sequential(\n",
        "            nn.GroupNorm(num_channels=embed_dim, eps=1e-5, affine=True, num_groups=1),\n",
        "            conv_2d(embed_dim, ffn_latent_dim, kernel_size=1, stride=1, bias=True, norm=False, act=True),\n",
        "            nn.Dropout(dropout),\n",
        "            conv_2d(ffn_latent_dim, embed_dim, kernel_size=1, stride=1, bias=True, norm=False, act=False),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self attention\n",
        "        x = x + self.pre_norm_attn(x)\n",
        "        # Feed Forward network\n",
        "        x = x + self.pre_norm_ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTBlockv2(nn.Module):\n",
        "    def __init__(self, inp, attn_dim, ffn_multiplier, attn_blocks, patch_size):\n",
        "        super(MobileViTBlockv2, self).__init__()\n",
        "        self.patch_h, self.patch_w = patch_size\n",
        "\n",
        "        # local representation\n",
        "        self.local_rep = nn.Sequential()\n",
        "        self.local_rep.add_module('conv_3x3', conv_2d(inp, inp, kernel_size=3, stride=1, padding=1, groups=inp))\n",
        "        self.local_rep.add_module('conv_1x1', conv_2d(inp, attn_dim, kernel_size=1, stride=1, norm=False, act=False))\n",
        "\n",
        "        # global representation\n",
        "        self.global_rep = nn.Sequential()\n",
        "        ffn_dims = [int((ffn_multiplier*attn_dim)//16*16)] * attn_blocks\n",
        "        for i in range(attn_blocks):\n",
        "            ffn_dim = ffn_dims[i]\n",
        "            self.global_rep.add_module(f'LinearAttnFFN_{i}', LinearAttnFFN(attn_dim, ffn_dim))\n",
        "        self.global_rep.add_module('LayerNorm2D', nn.GroupNorm(num_channels=attn_dim, eps=1e-5, affine=True, num_groups=1))\n",
        "\n",
        "        self.conv_proj = conv_2d(attn_dim, inp, kernel_size=1, stride=1, padding=0, act=False)\n",
        "\n",
        "    def unfolding_pytorch(self, feature_map):\n",
        "        batch_size, in_channels, img_h, img_w = feature_map.shape\n",
        "        # [B, C, H, W] --> [B, C, P, N]\n",
        "        patches = F.unfold(\n",
        "            feature_map,\n",
        "            kernel_size=(self.patch_h, self.patch_w),\n",
        "            stride=(self.patch_h, self.patch_w),\n",
        "        )\n",
        "        patches = patches.reshape(\n",
        "            batch_size, in_channels, self.patch_h * self.patch_w, -1\n",
        "        )\n",
        "        return patches, (img_h, img_w)\n",
        "\n",
        "    def folding_pytorch(self, patches, output_size):\n",
        "        batch_size, in_dim, patch_size, n_patches = patches.shape\n",
        "        # [B, C, P, N]\n",
        "        patches = patches.reshape(batch_size, in_dim * patch_size, n_patches)\n",
        "        feature_map = F.fold(\n",
        "            patches,\n",
        "            output_size=output_size,\n",
        "            kernel_size=(self.patch_h, self.patch_w),\n",
        "            stride=(self.patch_h, self.patch_w),\n",
        "        )\n",
        "        return feature_map\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.local_rep(x)\n",
        "        x, output_size = self.unfolding_pytorch(x)\n",
        "        x = self.global_rep(x)\n",
        "        x = self.folding_pytorch(patches=x, output_size=output_size)\n",
        "        x = self.conv_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTv2(nn.Module):\n",
        "    def __init__(self, image_size, width_multiplier, num_classes, patch_size=(2, 2)):\n",
        "        super().__init__()\n",
        "        # check image size\n",
        "        ih, iw = image_size\n",
        "        self.ph, self.pw = patch_size\n",
        "        assert ih % self.ph == 0 and iw % self.pw == 0\n",
        "        assert width_multiplier in [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]\n",
        "\n",
        "        # model size\n",
        "        channels = []\n",
        "        channels.append(int(max(16, min(64, 32 * width_multiplier))))\n",
        "        channels.append(int(64 * width_multiplier))\n",
        "        channels.append(int(128 * width_multiplier))\n",
        "        channels.append(int(256 * width_multiplier))\n",
        "        channels.append(int(384 * width_multiplier))\n",
        "        channels.append(int(512 * width_multiplier))\n",
        "        attn_dim = []\n",
        "        attn_dim.append(int(128 * width_multiplier))\n",
        "        attn_dim.append(int(192 * width_multiplier))\n",
        "        attn_dim.append(int(256 * width_multiplier))\n",
        "\n",
        "        # default shown in paper\n",
        "        ffn_multiplier = 2\n",
        "        mv2_exp_mult = 2\n",
        "\n",
        "        self.conv_0 = conv_2d(3, channels[0], kernel_size=3, stride=2)\n",
        "\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            InvertedResidual(channels[0], channels[1], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            InvertedResidual(channels[1], channels[2], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            InvertedResidual(channels[2], channels[3], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv2(channels[3], attn_dim[0], ffn_multiplier, 2, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            InvertedResidual(channels[3], channels[4], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv2(channels[4], attn_dim[1], ffn_multiplier, 4, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_5 = nn.Sequential(\n",
        "            InvertedResidual(channels[4], channels[5], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv2(channels[5], attn_dim[2], ffn_multiplier, 3, patch_size=patch_size)\n",
        "        )\n",
        "        self.out = nn.Linear(channels[-1], num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_0(x)\n",
        "        x = self.layer_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        x = self.layer_5(x)\n",
        "\n",
        "        # FF head\n",
        "        x = torch.mean(x, dim=[-2, -1])\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "k_24M70aCgbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "softmax_counter_attention_v2=[]\n",
        "softmax_counter_v2=0\n",
        "\n",
        "model = MobileViTv2(\n",
        "    image_size = (224,224),\n",
        "    width_multiplier = 2,             # support [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2] as shown in paper\n",
        "    num_classes=1000,\n",
        "    patch_size=(2, 2)\n",
        ")\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x) # (5, 1000)\n",
        "flop_analyzer = FlopCountAnalysis(model, x)\n",
        "print(flop_count_table(flop_analyzer))"
      ],
      "metadata": {
        "id": "VRxJyQnqD-8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_count=0\n",
        "for i in (softmax_counter_attention_v2):\n",
        "    print(i.shape)\n",
        "    softmax_count+=i.shape[0]*i.shape[1]*i.shape[2]\n",
        "print(\"\\ntotal number of times the softmax is used is :- \",softmax_count)"
      ],
      "metadata": {
        "id": "NPSFrQKIE2YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileViT v3"
      ],
      "metadata": {
        "id": "UBghMUBjGG4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mobilev3.1"
      ],
      "metadata": {
        "id": "4i8RQDjlGvsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "softmax_counter_attention_v3_1=[]\n",
        "softmax_counter_v3_1=0\n",
        "\n",
        "def conv_2d(inp, oup, kernel_size=3, stride=1, padding=0, groups=1, bias=False, norm=True, act=True):\n",
        "    conv = nn.Sequential()\n",
        "    conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
        "    if norm:\n",
        "        conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
        "    if act:\n",
        "        conv.add_module('Activation', nn.SiLU())\n",
        "    return conv\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "        # hidden_dim = int(round(inp * expand_ratio))\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.block = nn.Sequential()\n",
        "        if expand_ratio != 1:\n",
        "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0))\n",
        "        self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim))\n",
        "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, padding=0, act=False))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.block(x)\n",
        "        else:\n",
        "            return self.block(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=4, dim_head=8, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        b_sz, S_len, in_channels = x.shape\n",
        "        # self-attention\n",
        "        # [N, S, C] --> [N, S, 3C] --> [N, S, 3, h, c] where C = hc\n",
        "        qkv = self.qkv_proj(x).reshape(b_sz, S_len, 3, self.num_heads, -1)\n",
        "        # [N, S, 3, h, c] --> [N, h, 3, S, C]\n",
        "        qkv = qkv.transpose(1, 3).contiguous()\n",
        "        # [N, h, 3, S, C] --> [N, h, S, C] x 3\n",
        "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        # [N h, T, c] --> [N, h, c, T]\n",
        "        k = k.transpose(-1, -2)\n",
        "        # QK^T\n",
        "        # [N, h, S, c] x [N, h, c, T] --> [N, h, S, T]\n",
        "        attn = torch.matmul(q, k)\n",
        "        batch_size, num_heads, num_src_tokens, num_tgt_tokens = attn.shape\n",
        "        attn_dtype = attn.dtype\n",
        "        attn_as_float = self.softmax(attn.float())\n",
        "\n",
        "        global softmax_counter_attention_v3_1\n",
        "        softmax_counter_attention_v3_1.append(attn_as_float)\n",
        "        global softmax_counter_v3_1\n",
        "        softmax_counter_v3_1 += 1\n",
        "\n",
        "        attn = attn_as_float.to(attn_dtype)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # weighted sum\n",
        "        # [N, h, S, T] x [N, h, T, c] --> [N, h, S, c]\n",
        "        out = torch.matmul(attn, v)\n",
        "        # [N, h, S, c] --> [N, S, h, c] --> [N, S, C]\n",
        "        out = out.transpose(1, 2).reshape(b_sz, S_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, ffn_latent_dim, heads=8, dim_head=8, dropout=0, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.pre_norm_mha = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim, eps=1e-5, elementwise_affine=True),\n",
        "            Attention(embed_dim, heads, dim_head, attn_dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.pre_norm_ffn = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim, eps=1e-5, elementwise_affine=True),\n",
        "            nn.Linear(embed_dim, ffn_latent_dim, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ffn_latent_dim, embed_dim, bias=True),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention\n",
        "        x = x + self.pre_norm_mha(x)\n",
        "        # Feed Forward network\n",
        "        x = x + self.pre_norm_ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTBlockV3_v1(nn.Module):\n",
        "    def __init__(self, inp, attn_dim, ffn_multiplier, heads, dim_head, attn_blocks, patch_size):\n",
        "        super(MobileViTBlockV3_v1, self).__init__()\n",
        "        self.patch_h, self.patch_w = patch_size\n",
        "        self.patch_area = int(self.patch_h * self.patch_w)\n",
        "\n",
        "        # local representation\n",
        "        self.local_rep = nn.Sequential()\n",
        "        self.local_rep.add_module('conv_3x3', conv_2d(inp, inp, kernel_size=3, stride=1, padding=1, groups=inp))\n",
        "        self.local_rep.add_module('conv_1x1', conv_2d(inp, attn_dim, kernel_size=1, stride=1, norm=False, act=False))\n",
        "\n",
        "        # global representation\n",
        "        self.global_rep = nn.Sequential()\n",
        "        ffn_dims = [int((ffn_multiplier*attn_dim)//16*16)] * attn_blocks\n",
        "        for i in range(attn_blocks):\n",
        "            ffn_dim = ffn_dims[i]\n",
        "            self.global_rep.add_module(f'TransformerEncoder_{i}', TransformerEncoder(attn_dim, ffn_dim, heads, dim_head))\n",
        "        self.global_rep.add_module('LayerNorm', nn.LayerNorm(attn_dim, eps=1e-5, elementwise_affine=True))\n",
        "\n",
        "        self.conv_proj = conv_2d(attn_dim, inp, kernel_size=1, stride=1)\n",
        "        self.fusion = conv_2d(inp+attn_dim, inp, kernel_size=1, stride=1)\n",
        "\n",
        "    def unfolding(self, feature_map):\n",
        "        patch_w, patch_h = self.patch_w, self.patch_h\n",
        "        batch_size, in_channels, orig_h, orig_w = feature_map.shape\n",
        "\n",
        "        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)\n",
        "        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)\n",
        "\n",
        "        interpolate = False\n",
        "        if new_w != orig_w or new_h != orig_h:\n",
        "            # Note: Padding can be done, but then it needs to be handled in attention function.\n",
        "            feature_map = F.interpolate(\n",
        "                feature_map, size=(new_h, new_w), mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "            interpolate = True\n",
        "\n",
        "        # number of patches along width and height\n",
        "        num_patch_w = new_w // patch_w  # n_w\n",
        "        num_patch_h = new_h // patch_h  # n_h\n",
        "        num_patches = num_patch_h * num_patch_w  # N\n",
        "\n",
        "        # [B, C, H, W] --> [B * C * n_h, p_h, n_w, p_w]\n",
        "        reshaped_fm = feature_map.reshape(\n",
        "            batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w\n",
        "        )\n",
        "        # [B * C * n_h, p_h, n_w, p_w] --> [B * C * n_h, n_w, p_h, p_w]\n",
        "        transposed_fm = reshaped_fm.transpose(1, 2)\n",
        "        # [B * C * n_h, n_w, p_h, p_w] --> [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n",
        "        reshaped_fm = transposed_fm.reshape(\n",
        "            batch_size, in_channels, num_patches, self.patch_area\n",
        "        )\n",
        "        # [B, C, N, P] --> [B, P, N, C]\n",
        "        transposed_fm = reshaped_fm.transpose(1, 3)\n",
        "        # [B, P, N, C] --> [BP, N, C]\n",
        "        patches = transposed_fm.reshape(batch_size * self.patch_area, num_patches, -1)\n",
        "\n",
        "        info_dict = {\n",
        "            \"orig_size\": (orig_h, orig_w),\n",
        "            \"batch_size\": batch_size,\n",
        "            \"interpolate\": interpolate,\n",
        "            \"total_patches\": num_patches,\n",
        "            \"num_patches_w\": num_patch_w,\n",
        "            \"num_patches_h\": num_patch_h,\n",
        "        }\n",
        "\n",
        "        return patches, info_dict\n",
        "\n",
        "    def folding(self, patches, info_dict):\n",
        "        n_dim = patches.dim()\n",
        "        assert n_dim == 3, \"Tensor should be of shape BPxNxC. Got: {}\".format(\n",
        "            patches.shape\n",
        "        )\n",
        "        # [BP, N, C] --> [B, P, N, C]\n",
        "        patches = patches.contiguous().view(\n",
        "            info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1\n",
        "        )\n",
        "\n",
        "        batch_size, pixels, num_patches, channels = patches.size()\n",
        "        num_patch_h = info_dict[\"num_patches_h\"]\n",
        "        num_patch_w = info_dict[\"num_patches_w\"]\n",
        "\n",
        "        # [B, P, N, C] --> [B, C, N, P]\n",
        "        patches = patches.transpose(1, 3)\n",
        "\n",
        "        # [B, C, N, P] --> [B*C*n_h, n_w, p_h, p_w]\n",
        "        feature_map = patches.reshape(\n",
        "            batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w\n",
        "        )\n",
        "        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w]\n",
        "        feature_map = feature_map.transpose(1, 2)\n",
        "        # [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n",
        "        feature_map = feature_map.reshape(\n",
        "            batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w\n",
        "        )\n",
        "        if info_dict[\"interpolate\"]:\n",
        "            feature_map = F.interpolate(\n",
        "                feature_map,\n",
        "                size=info_dict[\"orig_size\"],\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "        return feature_map\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x.clone()\n",
        "        fm_conv = self.local_rep(x)\n",
        "        x, info_dict = self.unfolding(fm_conv)\n",
        "        x = self.global_rep(x)\n",
        "        x = self.folding(x, info_dict)\n",
        "        x = self.conv_proj(x)\n",
        "        x = self.fusion(torch.cat((fm_conv, x), dim=1))\n",
        "        x = x + res\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTv3_v1(nn.Module):\n",
        "    def __init__(self, image_size, mode, num_classes, patch_size=(2, 2)):\n",
        "        \"\"\"\n",
        "        Implementation of MobileViTv3 based on v1\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # check image size\n",
        "        ih, iw = image_size\n",
        "        self.ph, self.pw = patch_size\n",
        "        assert ih % self.ph == 0 and iw % self.pw == 0\n",
        "        assert mode in ['xx_small', 'x_small', 'small']\n",
        "\n",
        "        # model size\n",
        "        if mode == 'xx_small':\n",
        "            mv2_exp_mult = 2\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 4\n",
        "            channels = [16, 16, 24, 64, 80, 128]\n",
        "            attn_dim = [64, 80, 96]\n",
        "        elif mode == 'x_small':\n",
        "            mv2_exp_mult = 4\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 4\n",
        "            channels = [16, 32, 48, 96, 160, 160]\n",
        "            attn_dim = [96, 120, 144]\n",
        "        elif mode == 'small':\n",
        "            mv2_exp_mult = 4\n",
        "            ffn_multiplier = 2\n",
        "            last_layer_exp_factor = 3\n",
        "            channels = [16, 32, 64, 128, 256, 320]\n",
        "            attn_dim = [144, 192, 240]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.conv_0 = conv_2d(3, channels[0], kernel_size=3, stride=2)\n",
        "\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            InvertedResidual(channels[0], channels[1], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            InvertedResidual(channels[1], channels[2], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            InvertedResidual(channels[2], channels[3], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockV3_v1(channels[3], attn_dim[0], ffn_multiplier, heads=4, dim_head=8, attn_blocks=2, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            InvertedResidual(channels[3], channels[4], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockV3_v1(channels[4], attn_dim[1], ffn_multiplier, heads=4, dim_head=8, attn_blocks=4, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_5 = nn.Sequential(\n",
        "            InvertedResidual(channels[4], channels[5], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockV3_v1(channels[5], attn_dim[2], ffn_multiplier, heads=4, dim_head=8, attn_blocks=3, patch_size=patch_size)\n",
        "        )\n",
        "        self.conv_1x1_exp = conv_2d(channels[-1], channels[-1]*last_layer_exp_factor, kernel_size=1, stride=1)\n",
        "        self.out = nn.Linear(channels[-1]*last_layer_exp_factor, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_0(x)\n",
        "        x = self.layer_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        x = self.layer_5(x)\n",
        "        x = self.conv_1x1_exp(x)\n",
        "\n",
        "        # FF head\n",
        "        x = torch.mean(x, dim=[-2, -1])\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "vj7EvTYZFfzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "softmax_counter_attention_v3_1=[]\n",
        "softmax_counter_v3_1=0\n",
        "# MobileViTv3_v1 from (a)\n",
        "model = MobileViTv3_v1(\n",
        "    image_size = (224,224),\n",
        "    mode = 'small',             # support [\"xx_small\", \"x_small\", \"small\"] as shown in paper\n",
        "    num_classes=1000,\n",
        "    patch_size=(2, 2)\n",
        ")\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x) # (5, 1000)\n",
        "flop_analyzer = FlopCountAnalysis(model, x)\n",
        "print(flop_count_table(flop_analyzer))"
      ],
      "metadata": {
        "id": "-BgIbrd_GNGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_count=0\n",
        "for i in (softmax_counter_attention_v3_1):\n",
        "    print(i.shape)\n",
        "    softmax_count+=i.shape[0]*i.shape[1]*i.shape[2]\n",
        "print(\"\\ntotal number of times the softmax is used is :- \",softmax_count)"
      ],
      "metadata": {
        "id": "XNNRFuv_HmHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mobilevitv3.2\n"
      ],
      "metadata": {
        "id": "9V1hRYJ6G4bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "softmax_counter_attention_v3_2=[]\n",
        "softmax_counter_v3_2=0\n",
        "\n",
        "def conv_2d(inp, oup, kernel_size=3, stride=1, padding=0, groups=1, bias=False, norm=True, act=True):\n",
        "    conv = nn.Sequential()\n",
        "    conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
        "    if norm:\n",
        "        conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
        "    if act:\n",
        "        conv.add_module('Activation', nn.SiLU())\n",
        "    return conv\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "        # hidden_dim = int(round(inp * expand_ratio))\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.block = nn.Sequential()\n",
        "        if expand_ratio != 1:\n",
        "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=1, stride=1, padding=0))\n",
        "        self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim))\n",
        "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, padding=0, act=False))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.block(x)\n",
        "        else:\n",
        "            return self.block(x)\n",
        "\n",
        "\n",
        "class LinearSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = conv_2d(embed_dim, 1+2*embed_dim, kernel_size=1, bias=True, norm=False, act=False)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.out_proj = conv_2d(embed_dim, embed_dim, kernel_size=1, bias=True, norm=False, act=False)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = torch.split(qkv, split_size_or_sections=[1, self.embed_dim, self.embed_dim], dim=1)\n",
        "        context_score = F.softmax(q, dim=-1)\n",
        "        global softmax_counter_attention_v3_2\n",
        "        softmax_counter_attention_v3_2.append(context_score)\n",
        "        global softmax_counter_v3_2\n",
        "        softmax_counter_v3_2 += 1\n",
        "        context_score = self.attn_dropout(context_score)\n",
        "\n",
        "        context_vector = k * context_score\n",
        "        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)\n",
        "\n",
        "        out = F.relu(v) * context_vector.expand_as(v)\n",
        "        out = self.out_proj(out)\n",
        "        return out\n",
        "\n",
        "class LinearAttnFFN(nn.Module):\n",
        "    def __init__(self, embed_dim, ffn_latent_dim, dropout=0, attn_dropout=0):\n",
        "        super().__init__()\n",
        "        self.pre_norm_attn = nn.Sequential(\n",
        "            nn.GroupNorm(num_channels=embed_dim, eps=1e-5, affine=True, num_groups=1),\n",
        "            LinearSelfAttention(embed_dim, attn_dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.pre_norm_ffn = nn.Sequential(\n",
        "            nn.GroupNorm(num_channels=embed_dim, eps=1e-5, affine=True, num_groups=1),\n",
        "            conv_2d(embed_dim, ffn_latent_dim, kernel_size=1, stride=1, bias=True, norm=False, act=True),\n",
        "            nn.Dropout(dropout),\n",
        "            conv_2d(ffn_latent_dim, embed_dim, kernel_size=1, stride=1, bias=True, norm=False, act=False),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self attention\n",
        "        x = x + self.pre_norm_attn(x)\n",
        "        # Feed Forward network\n",
        "        x = x + self.pre_norm_ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTBlockv3_v2(nn.Module):\n",
        "    def __init__(self, inp, attn_dim, ffn_multiplier, attn_blocks, patch_size):\n",
        "        super(MobileViTBlockv3_v2, self).__init__()\n",
        "        self.patch_h, self.patch_w = patch_size\n",
        "\n",
        "        # local representation\n",
        "        self.local_rep = nn.Sequential()\n",
        "        self.local_rep.add_module('conv_3x3', conv_2d(inp, inp, kernel_size=3, stride=1, padding=1, groups=inp))\n",
        "        self.local_rep.add_module('conv_1x1', conv_2d(inp, attn_dim, kernel_size=1, stride=1, norm=False, act=False))\n",
        "\n",
        "        # global representation\n",
        "        self.global_rep = nn.Sequential()\n",
        "        ffn_dims = [int((ffn_multiplier*attn_dim)//16*16)] * attn_blocks\n",
        "        for i in range(attn_blocks):\n",
        "            ffn_dim = ffn_dims[i]\n",
        "            self.global_rep.add_module(f'LinearAttnFFN_{i}', LinearAttnFFN(attn_dim, ffn_dim))\n",
        "        self.global_rep.add_module('LayerNorm2D', nn.GroupNorm(num_channels=attn_dim, eps=1e-5, affine=True, num_groups=1))\n",
        "\n",
        "        self.conv_proj = conv_2d(2*attn_dim, inp, kernel_size=1, stride=1, padding=0, act=False)\n",
        "\n",
        "    def unfolding_pytorch(self, feature_map):\n",
        "        batch_size, in_channels, img_h, img_w = feature_map.shape\n",
        "        # [B, C, H, W] --> [B, C, P, N]\n",
        "        patches = F.unfold(\n",
        "            feature_map,\n",
        "            kernel_size=(self.patch_h, self.patch_w),\n",
        "            stride=(self.patch_h, self.patch_w),\n",
        "        )\n",
        "        patches = patches.reshape(\n",
        "            batch_size, in_channels, self.patch_h * self.patch_w, -1\n",
        "        )\n",
        "        return patches, (img_h, img_w)\n",
        "\n",
        "    def folding_pytorch(self, patches, output_size):\n",
        "        batch_size, in_dim, patch_size, n_patches = patches.shape\n",
        "        # [B, C, P, N]\n",
        "        patches = patches.reshape(batch_size, in_dim * patch_size, n_patches)\n",
        "        feature_map = F.fold(\n",
        "            patches,\n",
        "            output_size=output_size,\n",
        "            kernel_size=(self.patch_h, self.patch_w),\n",
        "            stride=(self.patch_h, self.patch_w),\n",
        "        )\n",
        "        return feature_map\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x.clone()\n",
        "        fm_conv = self.local_rep(x)\n",
        "        x, output_size = self.unfolding_pytorch(fm_conv)\n",
        "        x = self.global_rep(x)\n",
        "        x = self.folding_pytorch(patches=x, output_size=output_size)\n",
        "        x = self.conv_proj(torch.cat((x, fm_conv), dim=1))\n",
        "        x = x + res\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViTv3_v2(nn.Module):\n",
        "    def __init__(self, image_size, width_multiplier, num_classes, patch_size=(2, 2)):\n",
        "        \"\"\"\n",
        "        Implementation of MobileViTv3 based on v2\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # check image size\n",
        "        ih, iw = image_size\n",
        "        self.ph, self.pw = patch_size\n",
        "        assert ih % self.ph == 0 and iw % self.pw == 0\n",
        "        assert width_multiplier in [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]\n",
        "\n",
        "        # model size\n",
        "        channels = []\n",
        "        channels.append(int(max(16, min(64, 32 * width_multiplier))))\n",
        "        channels.append(int(64 * width_multiplier))\n",
        "        channels.append(int(128 * width_multiplier))\n",
        "        channels.append(int(256 * width_multiplier))\n",
        "        channels.append(int(384 * width_multiplier))\n",
        "        channels.append(int(512 * width_multiplier))\n",
        "        attn_dim = []\n",
        "        attn_dim.append(int(128 * width_multiplier))\n",
        "        attn_dim.append(int(192 * width_multiplier))\n",
        "        attn_dim.append(int(256 * width_multiplier))\n",
        "\n",
        "        # default shown in paper\n",
        "        ffn_multiplier = 2\n",
        "        mv2_exp_mult = 2\n",
        "\n",
        "        self.conv_0 = conv_2d(3, channels[0], kernel_size=3, stride=2)\n",
        "\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            InvertedResidual(channels[0], channels[1], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            InvertedResidual(channels[1], channels[2], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            InvertedResidual(channels[2], channels[2], stride=1, expand_ratio=mv2_exp_mult)\n",
        "        )\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            InvertedResidual(channels[2], channels[3], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv3_v2(channels[3], attn_dim[0], ffn_multiplier, 2, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            InvertedResidual(channels[3], channels[4], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv3_v2(channels[4], attn_dim[1], ffn_multiplier, 4, patch_size=patch_size)\n",
        "        )\n",
        "        self.layer_5 = nn.Sequential(\n",
        "            InvertedResidual(channels[4], channels[5], stride=2, expand_ratio=mv2_exp_mult),\n",
        "            MobileViTBlockv3_v2(channels[5], attn_dim[2], ffn_multiplier, 3, patch_size=patch_size)\n",
        "        )\n",
        "        self.out = nn.Linear(channels[-1], num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_0(x)\n",
        "        x = self.layer_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        x = self.layer_5(x)\n",
        "\n",
        "        # FF head\n",
        "        x = torch.mean(x, dim=[-2, -1])\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "sYkieLGTG_gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_counter_attention_v3_2=[]\n",
        "softmax_counter_v3_2=0\n",
        "\n",
        "# MobileViTv3_v2 from (b)\n",
        "model = MobileViTv3_v2(\n",
        "    image_size = (224,224),\n",
        "    width_multiplier = 1,       # support [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2] as shown in paper\n",
        "    num_classes=1000,\n",
        "    patch_size=(2, 2)\n",
        ")\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x) # (5, 1000)\n",
        "flop_analyzer = FlopCountAnalysis(model, x)\n",
        "print(flop_count_table(flop_analyzer))"
      ],
      "metadata": {
        "id": "hCfrEBwNG7EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_count=0\n",
        "for i in (softmax_counter_attention_v3_2):\n",
        "    print(i.shape)\n",
        "    softmax_count+=i.shape[0]*i.shape[1]*i.shape[2]\n",
        "print(\"\\ntotal number of times the softmax is used is :- \",softmax_count)"
      ],
      "metadata": {
        "id": "Fb08YJZCHHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kfYGdmdI1R2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}